{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2ca5bcb3",
      "metadata": {},
      "source": [
        "# Threshold Tuning for LinearSVC Multi-Label Genre Classification\n",
        "\n",
        "This notebook tunes prediction thresholds to optimize precision and F1 scores for the LinearSVC model.\n",
        "\n",
        "**Current Performance:**\n",
        "- Precision: 54.22%\n",
        "- Recall: 68.67%\n",
        "- F1 Score: 60.59%\n",
        "\n",
        "**Goal:** Find optimal threshold(s) to improve precision while maintaining reasonable recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dad56255",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and Setup\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.special import expit  # Sigmoid function for converting scores to probabilities\n",
        "from sklearn.metrics import (\n",
        "    f1_score, \n",
        "    precision_score, \n",
        "    recall_score, \n",
        "    hamming_loss, \n",
        "    jaccard_score\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Add project root to path\n",
        "sys.path.append('..')\n",
        "\n",
        "from descriptions.config import MODELS_DIR, INTERIM_DATA_DIR\n",
        "from descriptions.dataset import load_interim\n",
        "from descriptions.modeling.model import load_model\n",
        "from descriptions.modeling.preprocess import load_preprocessors\n",
        "from descriptions.modeling.train import prepare_features_and_labels, train_test_split_data\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"✓ Imports complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b914372",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and preprocessors\n",
        "print(\"Loading model and preprocessors...\")\n",
        "\n",
        "# Load LinearSVC model\n",
        "model_path = MODELS_DIR / \"linearsvc.joblib\"\n",
        "if not model_path.exists():\n",
        "    # Try to find any LinearSVC model\n",
        "    model_files = list(MODELS_DIR.glob(\"*linearsvc*.joblib\"))\n",
        "    if model_files:\n",
        "        model_path = model_files[0]\n",
        "        print(f\"Using model: {model_path.name}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"No LinearSVC model found in {MODELS_DIR}\")\n",
        "\n",
        "model = load_model(model_path)\n",
        "print(f\"✓ Model loaded: {model_path.name}\")\n",
        "\n",
        "# Load preprocessors\n",
        "vectorizer, mlb, feature_selector = load_preprocessors()\n",
        "print(f\"✓ Preprocessors loaded:\")\n",
        "print(f\"  - TfidfVectorizer: {vectorizer.max_features} features\")\n",
        "print(f\"  - MultiLabelBinarizer: {len(mlb.classes_)} genres\")\n",
        "print(f\"  - Feature Selector: {feature_selector.k if hasattr(feature_selector, 'k') else 'N/A'} features selected\")\n",
        "print(f\"\\nGenres: {list(mlb.classes_)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58262698",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare data\n",
        "print(\"Loading and preparing data...\")\n",
        "\n",
        "# Load interim data\n",
        "data = load_interim()\n",
        "print(f\"✓ Loaded {len(data)} samples\")\n",
        "\n",
        "# Prepare features and labels (same as training pipeline)\n",
        "# Function returns: (features_df, labels_array, vectorizer, mlb, feature_selector)\n",
        "# We already have the preprocessors, so we'll ignore the last 3 return values\n",
        "X, y, _, _, _ = prepare_features_and_labels(data, vectorizer, mlb, feature_selector)\n",
        "print(f\"✓ Features shape: {X.shape}\")\n",
        "print(f\"✓ Labels shape: {y.shape}\")\n",
        "\n",
        "# Split into train/test (use same random_state as training: 42)\n",
        "# We'll use test set for threshold tuning\n",
        "X_train, X_test, y_train, y_test = train_test_split_data(\n",
        "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Data split:\")\n",
        "print(f\"  - Training samples: {len(X_train)}\")\n",
        "print(f\"  - Test samples: {len(y_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f488a0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get prediction scores on test set\n",
        "print(\"Generating prediction scores on test set...\")\n",
        "\n",
        "# Convert to numpy array if needed\n",
        "X_test_array = X_test.values if isinstance(X_test, pd.DataFrame) else X_test\n",
        "\n",
        "# LinearSVC doesn't have predict_proba, so we use decision_function\n",
        "# decision_function returns raw scores (distance from hyperplane)\n",
        "# We'll convert these to probabilities using sigmoid function\n",
        "print(\"Getting decision function scores...\")\n",
        "y_scores = model.decision_function(X_test_array)\n",
        "print(f\"✓ Decision scores shape: {y_scores.shape}\")\n",
        "print(f\"✓ Score range: [{y_scores.min():.4f}, {y_scores.max():.4f}]\")\n",
        "print(f\"✓ Mean score: {y_scores.mean():.4f}\")\n",
        "\n",
        "# Convert scores to probabilities using sigmoid function\n",
        "# sigmoid(x) = 1 / (1 + exp(-x))\n",
        "# This converts scores to [0, 1] range\n",
        "y_proba = expit(y_scores)\n",
        "\n",
        "print(f\"\\n✓ Probabilities shape: {y_proba.shape}\")\n",
        "print(f\"✓ Probability range: [{y_proba.min():.4f}, {y_proba.max():.4f}]\")\n",
        "print(f\"✓ Mean probability: {y_proba.mean():.4f}\")\n",
        "\n",
        "# Show distribution of probabilities\n",
        "print(f\"\\nProbability distribution:\")\n",
        "print(f\"  - Mean: {y_proba.mean():.4f}\")\n",
        "print(f\"  - Median: {np.median(y_proba):.4f}\")\n",
        "print(f\"  - Std: {y_proba.std():.4f}\")\n",
        "print(f\"  - 25th percentile: {np.percentile(y_proba, 25):.4f}\")\n",
        "print(f\"  - 75th percentile: {np.percentile(y_proba, 75):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ef47f7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to evaluate metrics at a given threshold\n",
        "def evaluate_threshold(y_true, y_proba, threshold):\n",
        "    \"\"\"\n",
        "    Evaluate model performance at a given threshold.\n",
        "    \n",
        "    Args:\n",
        "        y_true: True binary labels (n_samples, n_labels)\n",
        "        y_proba: Prediction probabilities (n_samples, n_labels)\n",
        "        threshold: Probability threshold for binary predictions\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary of metrics\n",
        "    \"\"\"\n",
        "    # Apply threshold to get binary predictions\n",
        "    y_pred = (y_proba >= threshold).astype(int)\n",
        "    \n",
        "    # Calculate metrics (micro-averaged for multi-label)\n",
        "    metrics = {\n",
        "        'threshold': threshold,\n",
        "        'precision': precision_score(y_true, y_pred, average='micro', zero_division=0),\n",
        "        'recall': recall_score(y_true, y_pred, average='micro', zero_division=0),\n",
        "        'f1': f1_score(y_true, y_pred, average='micro', zero_division=0),\n",
        "        'hamming_loss': hamming_loss(y_true, y_pred),\n",
        "        'jaccard': jaccard_score(y_true, y_pred, average='micro', zero_division=0),\n",
        "    }\n",
        "    \n",
        "    # Calculate average number of predicted genres per sample\n",
        "    metrics['avg_predicted_genres'] = y_pred.sum(axis=1).mean()\n",
        "    metrics['avg_true_genres'] = y_true.sum(axis=1).mean()\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "print(\"✓ Evaluation function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d9ba11c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different thresholds\n",
        "print(\"Evaluating different thresholds...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Define threshold range to test\n",
        "thresholds = np.arange(0.3, 0.95, 0.05)\n",
        "results = []\n",
        "\n",
        "for threshold in tqdm(thresholds, desc=\"Testing thresholds\"):\n",
        "    metrics = evaluate_threshold(y_test, y_proba, threshold)\n",
        "    results.append(metrics)\n",
        "\n",
        "# Convert to DataFrame for easier analysis\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\n✓ Threshold evaluation complete!\")\n",
        "print(f\"\\nTested {len(thresholds)} thresholds from {thresholds.min():.2f} to {thresholds.max():.2f}\")\n",
        "print(\"\\nResults summary:\")\n",
        "print(results_df[['threshold', 'precision', 'recall', 'f1', 'hamming_loss']].to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e959b30a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find optimal thresholds for different objectives\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"OPTIMAL THRESHOLD ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# 1. Best F1 score\n",
        "best_f1_idx = results_df['f1'].idxmax()\n",
        "best_f1_threshold = results_df.loc[best_f1_idx, 'threshold']\n",
        "print(f\"\\n1. Best F1 Score:\")\n",
        "print(f\"   Threshold: {best_f1_threshold:.3f}\")\n",
        "print(f\"   F1: {results_df.loc[best_f1_idx, 'f1']:.4f} ({results_df.loc[best_f1_idx, 'f1']*100:.2f}%)\")\n",
        "print(f\"   Precision: {results_df.loc[best_f1_idx, 'precision']:.4f} ({results_df.loc[best_f1_idx, 'precision']*100:.2f}%)\")\n",
        "print(f\"   Recall: {results_df.loc[best_f1_idx, 'recall']:.4f} ({results_df.loc[best_f1_idx, 'recall']*100:.2f}%)\")\n",
        "print(f\"   Hamming Loss: {results_df.loc[best_f1_idx, 'hamming_loss']:.4f}\")\n",
        "\n",
        "# 2. Best precision (while maintaining reasonable recall > 0.5)\n",
        "precision_candidates = results_df[results_df['recall'] >= 0.5]\n",
        "if len(precision_candidates) > 0:\n",
        "    best_precision_idx = precision_candidates['precision'].idxmax()\n",
        "    best_precision_threshold = results_df.loc[best_precision_idx, 'threshold']\n",
        "    print(f\"\\n2. Best Precision (with recall >= 50%):\")\n",
        "    print(f\"   Threshold: {best_precision_threshold:.3f}\")\n",
        "    print(f\"   Precision: {results_df.loc[best_precision_idx, 'precision']:.4f} ({results_df.loc[best_precision_idx, 'precision']*100:.2f}%)\")\n",
        "    print(f\"   Recall: {results_df.loc[best_precision_idx, 'recall']:.4f} ({results_df.loc[best_precision_idx, 'recall']*100:.2f}%)\")\n",
        "    print(f\"   F1: {results_df.loc[best_precision_idx, 'f1']:.4f} ({results_df.loc[best_precision_idx, 'f1']*100:.2f}%)\")\n",
        "else:\n",
        "    print(\"\\n2. Best Precision: No threshold found with recall >= 50%\")\n",
        "\n",
        "# 3. Balanced precision/recall (closest to equal)\n",
        "results_df['precision_recall_diff'] = abs(results_df['precision'] - results_df['recall'])\n",
        "balanced_idx = results_df['precision_recall_diff'].idxmin()\n",
        "balanced_threshold = results_df.loc[balanced_idx, 'threshold']\n",
        "print(f\"\\n3. Most Balanced (Precision ≈ Recall):\")\n",
        "print(f\"   Threshold: {balanced_threshold:.3f}\")\n",
        "print(f\"   Precision: {results_df.loc[balanced_idx, 'precision']:.4f} ({results_df.loc[balanced_idx, 'precision']*100:.2f}%)\")\n",
        "print(f\"   Recall: {results_df.loc[balanced_idx, 'recall']:.4f} ({results_df.loc[balanced_idx, 'recall']*100:.2f}%)\")\n",
        "print(f\"   Difference: {results_df.loc[balanced_idx, 'precision_recall_diff']:.4f}\")\n",
        "print(f\"   F1: {results_df.loc[balanced_idx, 'f1']:.4f} ({results_df.loc[balanced_idx, 'f1']*100:.2f}%)\")\n",
        "\n",
        "# 4. Current threshold (0.5) performance\n",
        "current_idx_list = results_df[results_df['threshold'] == 0.5].index.tolist()\n",
        "if len(current_idx_list) > 0:\n",
        "    current_idx = current_idx_list[0]\n",
        "    print(f\"\\n4. Current Threshold (0.5):\")\n",
        "    print(f\"   Precision: {results_df.loc[current_idx, 'precision']:.4f} ({results_df.loc[current_idx, 'precision']*100:.2f}%)\")\n",
        "    print(f\"   Recall: {results_df.loc[current_idx, 'recall']:.4f} ({results_df.loc[current_idx, 'recall']*100:.2f}%)\")\n",
        "    print(f\"   F1: {results_df.loc[current_idx, 'f1']:.4f} ({results_df.loc[current_idx, 'f1']*100:.2f}%)\")\n",
        "    print(f\"   Hamming Loss: {results_df.loc[current_idx, 'hamming_loss']:.4f}\")\n",
        "else:\n",
        "    # Find closest to 0.5\n",
        "    current_idx = (results_df['threshold'] - 0.5).abs().idxmin()\n",
        "    current_thresh = results_df.loc[current_idx, 'threshold']\n",
        "    print(f\"\\n4. Closest Threshold to 0.5 ({current_thresh:.3f}):\")\n",
        "    print(f\"   Precision: {results_df.loc[current_idx, 'precision']:.4f} ({results_df.loc[current_idx, 'precision']*100:.2f}%)\")\n",
        "    print(f\"   Recall: {results_df.loc[current_idx, 'recall']:.4f} ({results_df.loc[current_idx, 'recall']*100:.2f}%)\")\n",
        "    print(f\"   F1: {results_df.loc[current_idx, 'f1']:.4f} ({results_df.loc[current_idx, 'f1']*100:.2f}%)\")\n",
        "    print(f\"   Hamming Loss: {results_df.loc[current_idx, 'hamming_loss']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e71726f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare variables for visualization\n",
        "# Find current threshold index (0.5 or closest)\n",
        "current_idx_list = results_df[results_df['threshold'] == 0.5].index.tolist()\n",
        "if len(current_idx_list) > 0:\n",
        "    current_idx = current_idx_list[0]\n",
        "else:\n",
        "    # Find closest to 0.5\n",
        "    current_idx = (results_df['threshold'] - 0.5).abs().idxmin()\n",
        "\n",
        "# Visualize threshold performance\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Threshold Tuning Analysis - LinearSVC Model', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot 1: Precision, Recall, F1 vs Threshold\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(results_df['threshold'], results_df['precision'], 'o-', label='Precision', linewidth=2, markersize=6)\n",
        "ax1.plot(results_df['threshold'], results_df['recall'], 's-', label='Recall', linewidth=2, markersize=6)\n",
        "ax1.plot(results_df['threshold'], results_df['f1'], '^-', label='F1 Score', linewidth=2, markersize=6)\n",
        "ax1.axvline(x=best_f1_threshold, color='r', linestyle='--', alpha=0.5, label=f'Best F1 ({best_f1_threshold:.2f})')\n",
        "ax1.axvline(x=0.5, color='g', linestyle='--', alpha=0.5, label='Current (0.5)')\n",
        "ax1.set_xlabel('Threshold', fontsize=12)\n",
        "ax1.set_ylabel('Score', fontsize=12)\n",
        "ax1.set_title('Precision, Recall, and F1 Score vs Threshold', fontsize=13, fontweight='bold')\n",
        "ax1.legend(loc='best')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Hamming Loss vs Threshold\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(results_df['threshold'], results_df['hamming_loss'], 'o-', color='purple', linewidth=2, markersize=6)\n",
        "ax2.axvline(x=best_f1_threshold, color='r', linestyle='--', alpha=0.5, label=f'Best F1 ({best_f1_threshold:.2f})')\n",
        "ax2.axvline(x=0.5, color='g', linestyle='--', alpha=0.5, label='Current (0.5)')\n",
        "ax2.set_xlabel('Threshold', fontsize=12)\n",
        "ax2.set_ylabel('Hamming Loss', fontsize=12)\n",
        "ax2.set_title('Hamming Loss vs Threshold', fontsize=13, fontweight='bold')\n",
        "ax2.legend(loc='best')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.invert_yaxis()  # Lower is better for Hamming Loss\n",
        "\n",
        "# Plot 3: Precision-Recall Trade-off\n",
        "ax3 = axes[1, 0]\n",
        "ax3.plot(results_df['recall'], results_df['precision'], 'o-', linewidth=2, markersize=6)\n",
        "ax3.scatter([results_df.loc[best_f1_idx, 'recall']], \n",
        "            [results_df.loc[best_f1_idx, 'precision']], \n",
        "            color='red', s=200, zorder=5, label=f'Best F1 ({best_f1_threshold:.2f})')\n",
        "ax3.scatter([results_df.loc[balanced_idx, 'recall']], \n",
        "            [results_df.loc[balanced_idx, 'precision']], \n",
        "            color='orange', s=200, zorder=5, label=f'Balanced ({balanced_threshold:.2f})')\n",
        "ax3.scatter([results_df.loc[current_idx, 'recall']], \n",
        "            [results_df.loc[current_idx, 'precision']], \n",
        "            color='green', s=200, zorder=5, label='Current (0.5)')\n",
        "ax3.set_xlabel('Recall', fontsize=12)\n",
        "ax3.set_ylabel('Precision', fontsize=12)\n",
        "ax3.set_title('Precision-Recall Trade-off', fontsize=13, fontweight='bold')\n",
        "ax3.legend(loc='best')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Average Genres per Sample\n",
        "ax4 = axes[1, 1]\n",
        "ax4.plot(results_df['threshold'], results_df['avg_predicted_genres'], 'o-', label='Predicted', linewidth=2, markersize=6)\n",
        "ax4.axhline(y=results_df['avg_true_genres'].iloc[0], color='r', linestyle='--', label='True Average', linewidth=2)\n",
        "ax4.axvline(x=best_f1_threshold, color='r', linestyle='--', alpha=0.3, label=f'Best F1 ({best_f1_threshold:.2f})')\n",
        "ax4.axvline(x=0.5, color='g', linestyle='--', alpha=0.3, label='Current (0.5)')\n",
        "ax4.set_xlabel('Threshold', fontsize=12)\n",
        "ax4.set_ylabel('Average Genres per Sample', fontsize=12)\n",
        "ax4.set_title('Average Number of Predicted Genres vs Threshold', fontsize=13, fontweight='bold')\n",
        "ax4.legend(loc='best')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Visualizations complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b866ad12",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed comparison table\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DETAILED THRESHOLD COMPARISON\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Select key thresholds to compare\n",
        "key_thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, best_f1_threshold, balanced_threshold]\n",
        "key_thresholds = sorted(set([round(t, 2) for t in key_thresholds]))\n",
        "\n",
        "comparison_data = []\n",
        "for thresh in key_thresholds:\n",
        "    # Find closest threshold in results\n",
        "    closest_idx = (results_df['threshold'] - thresh).abs().idxmin()\n",
        "    row = results_df.loc[closest_idx].copy()\n",
        "    row['threshold'] = thresh  # Use exact threshold value\n",
        "    comparison_data.append(row)\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df = comparison_df.round(4)\n",
        "\n",
        "print(\"\\nKey Metrics by Threshold:\")\n",
        "print(comparison_df[['threshold', 'precision', 'recall', 'f1', 'hamming_loss', 'avg_predicted_genres']].to_string(index=False))\n",
        "\n",
        "# Save results to CSV\n",
        "output_path = Path(\"../reports/threshold_tuning_results.csv\")\n",
        "comparison_df.to_csv(output_path, index=False)\n",
        "print(f\"\\n✓ Results saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65e487ea",
      "metadata": {},
      "source": [
        "## Recommendations\n",
        "\n",
        "Based on the threshold tuning analysis above, review the optimal thresholds found in the previous cells:\n",
        "\n",
        "1. **For Best F1 Score**: Use the threshold identified in the analysis\n",
        "   - This maximizes overall F1 score\n",
        "   - Balance between precision and recall\n",
        "\n",
        "2. **For Better Precision**: Use the threshold that maximizes precision while maintaining recall >= 50%\n",
        "   - Improves precision while maintaining reasonable recall\n",
        "   - Reduces false positives\n",
        "\n",
        "3. **For Balanced Performance**: Use the threshold with most balanced precision/recall\n",
        "   - Most balanced precision/recall trade-off\n",
        "   - Good for general use cases\n",
        "\n",
        "4. **Current Performance**: Threshold = 0.5\n",
        "   - Compare current metrics with optimal thresholds found above\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. **Test optimal threshold in production**: Update `predict.py` to use the recommended threshold\n",
        "2. **Per-genre threshold tuning**: Consider optimizing thresholds per genre (see next cell)\n",
        "3. **Monitor performance**: Track metrics when using new threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6773d26",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Per-genre threshold optimization\n",
        "print(\"=\" * 70)\n",
        "print(\"PER-GENRE THRESHOLD OPTIMIZATION\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nFinding optimal threshold for each genre individually...\\n\")\n",
        "\n",
        "genre_thresholds = {}\n",
        "genre_results = []\n",
        "\n",
        "for genre_idx, genre_name in enumerate(mlb.classes_):\n",
        "    best_f1 = 0\n",
        "    best_thresh = 0.5\n",
        "    \n",
        "    # Test thresholds for this genre\n",
        "    for threshold in np.arange(0.2, 0.9, 0.05):\n",
        "        y_pred_genre = (y_proba[:, genre_idx] >= threshold).astype(int)\n",
        "        y_true_genre = y_test[:, genre_idx]\n",
        "        \n",
        "        if y_true_genre.sum() == 0:  # Skip if no true labels for this genre\n",
        "            continue\n",
        "            \n",
        "        precision = precision_score(y_true_genre, y_pred_genre, zero_division=0)\n",
        "        recall = recall_score(y_true_genre, y_pred_genre, zero_division=0)\n",
        "        \n",
        "        if precision + recall > 0:\n",
        "            f1 = 2 * (precision * recall) / (precision + recall)\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_thresh = threshold\n",
        "    \n",
        "    genre_thresholds[genre_name] = best_thresh\n",
        "    genre_results.append({\n",
        "        'genre': genre_name,\n",
        "        'optimal_threshold': best_thresh,\n",
        "        'f1_at_optimal': best_f1,\n",
        "        'f1_at_0.5': f1_score(y_test[:, genre_idx], \n",
        "                              (y_proba[:, genre_idx] >= 0.5).astype(int), \n",
        "                              zero_division=0)\n",
        "    })\n",
        "\n",
        "genre_df = pd.DataFrame(genre_results)\n",
        "genre_df = genre_df.sort_values('optimal_threshold', ascending=False)\n",
        "\n",
        "print(\"Optimal thresholds per genre:\")\n",
        "print(genre_df.to_string(index=False))\n",
        "\n",
        "# Show genres that benefit most from threshold tuning\n",
        "genre_df['improvement'] = genre_df['f1_at_optimal'] - genre_df['f1_at_0.5']\n",
        "top_improvements = genre_df.nlargest(10, 'improvement')\n",
        "print(f\"\\nTop 10 genres with most improvement from threshold tuning:\")\n",
        "print(top_improvements[['genre', 'optimal_threshold', 'f1_at_optimal', 'f1_at_0.5', 'improvement']].to_string(index=False))\n",
        "\n",
        "# Save per-genre thresholds\n",
        "genre_thresholds_path = Path(\"../models/genre_thresholds.json\")\n",
        "import json\n",
        "with open(genre_thresholds_path, 'w') as f:\n",
        "    json.dump(genre_thresholds, f, indent=2)\n",
        "print(f\"\\n✓ Per-genre thresholds saved to {genre_thresholds_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe5410b8",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook evaluated different thresholds for the LinearSVC model and identified optimal thresholds for various objectives:\n",
        "\n",
        "- **Best F1 Score**: Found optimal threshold for overall performance\n",
        "- **Precision Optimization**: Identified thresholds that improve precision while maintaining recall\n",
        "- **Balanced Performance**: Found threshold with most balanced precision/recall\n",
        "- **Per-Genre Analysis**: Optimized thresholds for individual genres\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "1. **Current threshold (0.5)** performance vs optimal thresholds\n",
        "2. **Precision can be improved** by increasing threshold\n",
        "3. **Trade-off analysis** between precision and recall\n",
        "4. **Per-genre thresholds** may provide additional improvements\n",
        "\n",
        "### Usage:\n",
        "\n",
        "To use the optimal threshold in predictions:\n",
        "\n",
        "```python\n",
        "python -m descriptions.modeling.predict \\\n",
        "    --description \"Your movie description here\" \\\n",
        "    --threshold <optimal_threshold>\n",
        "```\n",
        "\n",
        "Or update the default threshold in `predict.py` to use the recommended value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "109b588d",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
