{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "overfitting-notebook-header",
      "metadata": {},
      "source": [
        "# Overfitting Testing Notebook\n",
        "\n",
        "This notebook is dedicated to testing and analyzing overfitting in the movie genre classification model.\n",
        "\n",
        "## Features:\n",
        "- Train models with different regularization levels\n",
        "- Compare train vs test performance across different configurations\n",
        "- Visualize overfitting metrics and learning curves\n",
        "- Test different regularization strategies (L1, L2, ElasticNet)\n",
        "- Analyze performance across different training set sizes\n",
        "- Test TruncatedSVD dimensionality reduction to reduce overfitting\n",
        "- Identify optimal regularization parameters to minimize overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "imports",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ All imports successful\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, learning_curve, \n",
        "    validation_curve, cross_val_score, KFold)\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    f1_score, precision_score, recall_score, \n",
        "    hamming_loss, jaccard_score, make_scorer\n",
        ")\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "import time\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import json\n",
        "\n",
        "# Project imports\n",
        "from descriptions.config import INTERIM_DATA_DIR, MODELS_DIR\n",
        "from descriptions.dataset import load_interim\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(\"✓ All imports successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load-data-header",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "load-data",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "\u001b[32m2025-12-09 15:58:35.293\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.dataset\u001b[0m:\u001b[36mload_interim\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mLoading interim data from /Users/christianfullerton/Developer/Python Workspace/movie_genre_model/data/interim/cleaned_movies.csv...\u001b[0m\n",
            "\u001b[32m2025-12-09 15:58:35.365\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.dataset\u001b[0m:\u001b[36mload_interim\u001b[0m:\u001b[36m103\u001b[0m - \u001b[34m\u001b[1mLoaded with index column\u001b[0m\n",
            "\u001b[32m2025-12-09 15:58:35.366\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdescriptions.dataset\u001b[0m:\u001b[36mload_interim\u001b[0m:\u001b[36m108\u001b[0m - \u001b[32m\u001b[1m✓ Data loaded successfully: 9087 rows, 2 columns\u001b[0m\n",
            "✓ Loaded 9087 samples\n",
            "Columns: ['genre', 'description']\n",
            "\n",
            "Splitting data...\n",
            "✓ Data split complete: 7269 training samples, 1818 test samples\n"
          ]
        }
      ],
      "source": [
        "# Load and prepare data\n",
        "print(\"Loading data...\")\n",
        "data = load_interim(INTERIM_DATA_DIR / \"cleaned_movies.csv\")\n",
        "print(f\"✓ Loaded {len(data)} samples\")\n",
        "print(f\"Columns: {list(data.columns)}\")\n",
        "\n",
        "# Split data into train and test sets BEFORE preprocessing (prevents data leakage)\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n",
        "X_text, y_genres = data['description'], data['genre']\n",
        "\n",
        "print(\"\\nSplitting data...\")\n",
        "X_train_text, X_test_text, y_train_genres, y_test_genres = train_test_split(\n",
        "    X_text, y_genres, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
        ")\n",
        "print(f\"✓ Data split complete: {len(X_train_text)} training samples, {len(X_test_text)} test samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "evaluation-function-header",
      "metadata": {},
      "source": [
        "## 2. Evaluation Function\n",
        "\n",
        "Helper function to evaluate models and calculate metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1251ab9f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total unique genres found: 37\n",
            "Genres: {'Crime', 'Science Fiction', ' Adventure', ' Action', ' Horror', ' Family', 'War', 'History', ' Fantasy', 'Drama', ' Crime', 'Comedy', 'Thriller', 'Romance', nan, 'Action', ' History', 'Horror', ' Mystery', 'Family', 'Fantasy', 'Music', 'TV Movie', 'Mystery', ' War', ' TV Movie', ' Science Fiction', ' Animation', ' Comedy', 'Western', ' Romance', ' Thriller', 'Adventure', 'Animation', ' Western', ' Music', ' Drama'}\n"
          ]
        }
      ],
      "source": [
        "# Get unique genres for exploration (from all data)\n",
        "unique_genres = set(y_genres.str.split(\",\").explode().unique())\n",
        "print(f\"Total unique genres found: {len(unique_genres)}\")\n",
        "print(f\"Genres: {unique_genres}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f51262e",
      "metadata": {},
      "source": [
        "## 3. Preprocess Data\n",
        "\n",
        "Preprocess genres and generate TF-IDF features for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9ed05454",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing genres...\n",
            "\n",
            "Analyzing genre frequencies...\n",
            "  Removing 'Music': 2.92% (212 samples)\n",
            "  Removing 'TV Movie': 1.09% (79 samples)\n",
            "  Removing 'War': 3.44% (250 samples)\n",
            "  Removing 'Western': 1.40% (102 samples)\n",
            "\n",
            "✓ Identified 4 genres to remove (< 5.0%)\n",
            "\n",
            "Filtering out rare genres from genre lists...\n",
            "  ⚠️  Warning: 16 training samples and 11 test samples lost all genres\n",
            "  These samples will be removed from the dataset\n",
            "  Removed 16 training samples with no genres\n",
            "  Removed 11 test samples with no genres\n",
            "\n",
            "Fitting MultiLabelBinarizer on filtered training data...\n",
            "✓ Binary labels created:\n",
            "  Training: 7253 samples × 14 labels\n",
            "  Test: 1807 samples × 14 labels\n",
            "  Unique genres: 14\n",
            "  Genres kept: ['Action', 'Adventure', 'Animation', 'Comedy', 'Crime', 'Drama', 'Family', 'Fantasy', 'History', 'Horror', 'Mystery', 'Romance', 'Science Fiction', 'Thriller']\n",
            "\n",
            "Updated genre distribution:\n",
            "  Action              :  1663 samples (22.93%)\n",
            "  Adventure           :  1204 samples (16.60%)\n",
            "  Animation           :   697 samples ( 9.61%)\n",
            "  Comedy              :  2673 samples (36.85%)\n",
            "  Crime               :  1183 samples (16.31%)\n",
            "  Drama               :  3431 samples (47.30%)\n",
            "  Family              :   823 samples (11.35%)\n",
            "  Fantasy             :   814 samples (11.22%)\n",
            "  History             :   367 samples ( 5.06%)\n",
            "  Horror              :   959 samples (13.22%)\n",
            "  Mystery             :   704 samples ( 9.71%)\n",
            "  Romance             :  1292 samples (17.81%)\n",
            "  Science Fiction     :   823 samples (11.35%)\n",
            "  Thriller            :  1933 samples (26.65%)\n",
            "\n",
            "  New imbalance ratio: 9.3x (was 43.4x)\n",
            "\n",
            "Generating TF-IDF features...\n",
            "✓ TF-IDF features created:\n",
            "  Training: 7253 samples × 10000 features\n",
            "  Test: 1807 samples × 10000 features\n"
          ]
        }
      ],
      "source": [
        "# Preprocess genres: convert comma-separated strings to lists\n",
        "print(\"Preprocessing genres...\")\n",
        "\n",
        "# Training genres\n",
        "y_train_genres_list = y_train_genres.fillna(\"\").astype(str).str.strip()\n",
        "y_train_genres_list = y_train_genres_list.str.split(r\"\\s*,\\s*\").apply(\n",
        "    lambda genres: sorted({g.strip() for g in genres if g.strip()})\n",
        ")\n",
        "\n",
        "# Test genres\n",
        "y_test_genres_list = y_test_genres.fillna(\"\").astype(str).str.strip()\n",
        "y_test_genres_list = y_test_genres_list.str.split(r\"\\s*,\\s*\").apply(\n",
        "    lambda genres: sorted({g.strip() for g in genres if g.strip()})\n",
        ")\n",
        "\n",
        "# Step 1: Analyze genre frequencies to identify rare genres\n",
        "print(\"\\nAnalyzing genre frequencies...\")\n",
        "mlb_temp = MultiLabelBinarizer()\n",
        "y_train_binary_temp = mlb_temp.fit_transform(y_train_genres_list)\n",
        "\n",
        "# Calculate genre frequencies and percentages\n",
        "genre_counts = y_train_binary_temp.sum(axis=0)\n",
        "genre_percentages = (genre_counts / len(y_train_binary_temp)) * 100\n",
        "\n",
        "# Identify genres to remove (< 5%)\n",
        "MIN_PERCENTAGE = 5.0\n",
        "genres_to_remove = set()\n",
        "for i, (genre, percentage) in enumerate(zip(mlb_temp.classes_, genre_percentages)):\n",
        "    if percentage < MIN_PERCENTAGE:\n",
        "        genres_to_remove.add(genre)\n",
        "        print(f\"  Removing '{genre}': {percentage:.2f}% ({int(genre_counts[i])} samples)\")\n",
        "\n",
        "print(f\"\\n✓ Identified {len(genres_to_remove)} genres to remove (< {MIN_PERCENTAGE}%)\")\n",
        "\n",
        "# Step 2: Filter out rare genres from genre lists\n",
        "print(\"\\nFiltering out rare genres from genre lists...\")\n",
        "y_train_genres_filtered = y_train_genres_list.apply(\n",
        "    lambda genres: sorted({g for g in genres if g not in genres_to_remove})\n",
        ")\n",
        "y_test_genres_filtered = y_test_genres_list.apply(\n",
        "    lambda genres: sorted({g for g in genres if g not in genres_to_remove})\n",
        ")\n",
        "\n",
        "# Count how many samples lost all genres\n",
        "train_samples_lost = (y_train_genres_filtered.apply(len) == 0).sum()\n",
        "test_samples_lost = (y_test_genres_filtered.apply(len) == 0).sum()\n",
        "\n",
        "if train_samples_lost > 0 or test_samples_lost > 0:\n",
        "    print(f\"  ⚠️  Warning: {train_samples_lost} training samples and {test_samples_lost} test samples lost all genres\")\n",
        "    print(f\"  These samples will be removed from the dataset\")\n",
        "\n",
        "# Remove samples that have no genres left\n",
        "if train_samples_lost > 0:\n",
        "    keep_train_mask = y_train_genres_filtered.apply(len) > 0\n",
        "    y_train_genres_filtered = y_train_genres_filtered[keep_train_mask]\n",
        "    X_train_text = X_train_text[keep_train_mask]\n",
        "    print(f\"  Removed {train_samples_lost} training samples with no genres\")\n",
        "\n",
        "if test_samples_lost > 0:\n",
        "    keep_test_mask = y_test_genres_filtered.apply(len) > 0\n",
        "    y_test_genres_filtered = y_test_genres_filtered[keep_test_mask]\n",
        "    X_test_text = X_test_text[keep_test_mask]\n",
        "    print(f\"  Removed {test_samples_lost} test samples with no genres\")\n",
        "\n",
        "# Step 3: Fit MultiLabelBinarizer on filtered training data\n",
        "print(\"\\nFitting MultiLabelBinarizer on filtered training data...\")\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_train_binary = mlb.fit_transform(y_train_genres_filtered)\n",
        "y_test_binary = mlb.transform(y_test_genres_filtered)  # Transform test data\n",
        "\n",
        "print(f\"✓ Binary labels created:\")\n",
        "print(f\"  Training: {y_train_binary.shape[0]} samples × {y_train_binary.shape[1]} labels\")\n",
        "print(f\"  Test: {y_test_binary.shape[0]} samples × {y_test_binary.shape[1]} labels\")\n",
        "print(f\"  Unique genres: {len(mlb.classes_)}\")\n",
        "print(f\"  Genres kept: {list(mlb.classes_)}\")\n",
        "\n",
        "# Show updated genre distribution\n",
        "print(\"\\nUpdated genre distribution:\")\n",
        "genre_counts_new = y_train_binary.sum(axis=0)\n",
        "genre_percentages_new = (genre_counts_new / len(y_train_binary)) * 100\n",
        "for genre, count, pct in zip(mlb.classes_, genre_counts_new, genre_percentages_new):\n",
        "    print(f\"  {genre:20s}: {count:5d} samples ({pct:5.2f}%)\")\n",
        "\n",
        "# Calculate new imbalance ratio\n",
        "max_count_new = genre_counts_new.max()\n",
        "min_count_new = genre_counts_new.min()\n",
        "imbalance_ratio_new = max_count_new / min_count_new if min_count_new > 0 else float('inf')\n",
        "print(f\"\\n  New imbalance ratio: {imbalance_ratio_new:.1f}x (was 43.4x)\")\n",
        "\n",
        "# Generate TF-IDF features\n",
        "print(\"\\nGenerating TF-IDF features...\")\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=10000,\n",
        "    stop_words=\"english\",\n",
        "    ngram_range=(1, 2),\n",
        "    sublinear_tf=True,\n",
        "    max_df=0.7,\n",
        "    min_df=3,\n",
        "    use_idf=True\n",
        ")\n",
        "\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train_text)\n",
        "X_test_tfidf = vectorizer.transform(X_test_text)\n",
        "\n",
        "print(f\"✓ TF-IDF features created:\")\n",
        "print(f\"  Training: {X_train_tfidf.shape[0]} samples × {X_train_tfidf.shape[1]} features\")\n",
        "print(f\"  Test: {X_test_tfidf.shape[0]} samples × {X_test_tfidf.shape[1]} features\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feature-selection-header",
      "metadata": {},
      "source": [
        "## 3.5. Feature Selection with SelectKBest\n",
        "\n",
        "Select the most informative features to reduce overfitting and improve generalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "feature-selection",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying feature selection with SelectKBest...\n",
            "✓ Feature selection complete:\n",
            "  Original features: 10000\n",
            "  Selected features: 7000\n",
            "  Features removed: 3000\n",
            "  Reduction: 30.0%\n",
            "\n",
            "  Training: 7253 samples × 7000 features\n",
            "  Test: 1807 samples × 7000 features\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Applying feature selection with SelectKBest...\")\n",
        "K_FEATURES = 7000  \n",
        "feature_selector = SelectKBest(score_func=chi2, k=K_FEATURES)\n",
        "\n",
        "# Fit on training data and transform both train and test\n",
        "# IMPORTANT: Fit only on training data to prevent data leakage\n",
        "X_train_selected = feature_selector.fit_transform(X_train_tfidf, y_train_binary)\n",
        "X_test_selected = feature_selector.transform(X_test_tfidf)\n",
        "\n",
        "print(f\"✓ Feature selection complete:\")\n",
        "print(f\"  Original features: {X_train_tfidf.shape[1]}\")\n",
        "print(f\"  Selected features: {X_train_selected.shape[1]}\")\n",
        "print(f\"  Features removed: {X_train_tfidf.shape[1] - X_train_selected.shape[1]}\")\n",
        "print(f\"  Reduction: {(1 - X_train_selected.shape[1]/X_train_tfidf.shape[1])*100:.1f}%\")\n",
        "print(f\"\\n  Training: {X_train_selected.shape[0]} samples × {X_train_selected.shape[1]} features\")\n",
        "print(f\"  Test: {X_test_selected.shape[0]} samples × {X_test_selected.shape[1]} features\")\n",
        "\n",
        "# Note: Use X_train_selected and X_test_selected for training instead of X_train_tfidf and X_test_tfidf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b5c4447",
      "metadata": {},
      "source": [
        "## 4. Evaluation Function\n",
        "\n",
        "Helper function to evaluate models and calculate metrics for overfitting analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9d001e2d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Evaluation function defined\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate a model on both training and test sets.\n",
        "    \n",
        "    Returns a dictionary with train and test metrics.\n",
        "    \"\"\"\n",
        "    # Predictions\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'train': {\n",
        "            'f1_macro': f1_score(y_train, y_train_pred, average='macro', zero_division=0),\n",
        "            'f1_micro': f1_score(y_train, y_train_pred, average='micro', zero_division=0),\n",
        "            'precision_macro': precision_score(y_train, y_train_pred, average='macro', zero_division=0),\n",
        "            'recall_macro': recall_score(y_train, y_train_pred, average='macro', zero_division=0),\n",
        "            'hamming_loss': hamming_loss(y_train, y_train_pred),\n",
        "            'jaccard_score': jaccard_score(y_train, y_train_pred, average='macro', zero_division=0)\n",
        "        },\n",
        "        'test': {\n",
        "            'f1_macro': f1_score(y_test, y_test_pred, average='macro', zero_division=0),\n",
        "            'f1_micro': f1_score(y_test, y_test_pred, average='micro', zero_division=0),\n",
        "            'precision_macro': precision_score(y_test, y_test_pred, average='macro', zero_division=0),\n",
        "            'recall_macro': recall_score(y_test, y_test_pred, average='macro', zero_division=0),\n",
        "            'hamming_loss': hamming_loss(y_test, y_test_pred),\n",
        "            'jaccard_score': jaccard_score(y_test, y_test_pred, average='macro', zero_division=0)\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Calculate overfitting gap (train - test)\n",
        "    metrics['overfitting_gap'] = {\n",
        "        'f1_macro': metrics['train']['f1_macro'] - metrics['test']['f1_macro'],\n",
        "        'f1_micro': metrics['train']['f1_micro'] - metrics['test']['f1_micro'],\n",
        "        'precision_macro': metrics['train']['precision_macro'] - metrics['test']['precision_macro'],\n",
        "        'recall_macro': metrics['train']['recall_macro'] - metrics['test']['recall_macro'],\n",
        "        'hamming_loss': metrics['test']['hamming_loss'] - metrics['train']['hamming_loss'],  # Lower is better\n",
        "        'jaccard_score': metrics['train']['jaccard_score'] - metrics['test']['jaccard_score']\n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "print(\"✓ Evaluation function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac54bce0",
      "metadata": {},
      "source": [
        "## 5. Test Different Alpha Values for Overfitting\n",
        "\n",
        "Train models with varying regularization strength (alpha) to analyze overfitting.\n",
        "Smaller alpha = less regularization = more overfitting risk.\n",
        "Larger alpha = more regularization = less overfitting risk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "636c82d8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing 1 different C values (LogisticRegression):\n",
            "C values: [1]\n",
            "\n",
            "\n",
            "Training LogisticRegression with C=1.00...\n",
            "  Performing 5-fold cross-validation...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  CV F1 Macro: 0.5470 (+/- 0.0070)\n",
            "  ✓ Completed in 498.32s - Test F1 Macro: 0.5628\n",
            "\n",
            "✓ C value testing with cross-validation complete!\n",
            "\n",
            "Results summary:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>C</th>\n",
              "      <th>cv_f1_macro_mean</th>\n",
              "      <th>cv_f1_macro_std</th>\n",
              "      <th>cv_f1_macro_min</th>\n",
              "      <th>cv_f1_macro_max</th>\n",
              "      <th>train_f1_macro</th>\n",
              "      <th>test_f1_macro</th>\n",
              "      <th>train_f1_micro</th>\n",
              "      <th>test_f1_micro</th>\n",
              "      <th>train_hamming_loss</th>\n",
              "      <th>test_hamming_loss</th>\n",
              "      <th>overfitting_gap_f1_macro</th>\n",
              "      <th>overfitting_gap_f1_micro</th>\n",
              "      <th>training_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.546959</td>\n",
              "      <td>0.006985</td>\n",
              "      <td>0.53803</td>\n",
              "      <td>0.556678</td>\n",
              "      <td>0.717693</td>\n",
              "      <td>0.56281</td>\n",
              "      <td>0.733524</td>\n",
              "      <td>0.59262</td>\n",
              "      <td>0.113884</td>\n",
              "      <td>0.174559</td>\n",
              "      <td>0.154883</td>\n",
              "      <td>0.140904</td>\n",
              "      <td>498.323434</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   C  cv_f1_macro_mean  cv_f1_macro_std  cv_f1_macro_min  cv_f1_macro_max  \\\n",
              "0  1          0.546959         0.006985          0.53803         0.556678   \n",
              "\n",
              "   train_f1_macro  test_f1_macro  train_f1_micro  test_f1_micro  \\\n",
              "0        0.717693        0.56281        0.733524        0.59262   \n",
              "\n",
              "   train_hamming_loss  test_hamming_loss  overfitting_gap_f1_macro  \\\n",
              "0            0.113884           0.174559                  0.154883   \n",
              "\n",
              "   overfitting_gap_f1_micro  training_time  \n",
              "0                  0.140904     498.323434  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Best C (highest CV F1 Macro): 1.00\n",
            "  CV F1 Macro: 0.5470 (+/- 0.0070)\n",
            "  Test F1 Macro: 0.5628\n"
          ]
        }
      ],
      "source": [
        "# Convert alpha values to C values (LogisticRegression uses C = 1/alpha)\n",
        "# Test a range of C values (larger C = less regularization)\n",
        "C_values = [1]\n",
        "\n",
        "print(f\"Testing {len(C_values)} different C values (LogisticRegression):\")\n",
        "print(f\"C values: {C_values}\\n\")\n",
        "\n",
        "# Create custom scorer for multi-label F1 macro\n",
        "f1_macro_scorer = make_scorer(f1_score, average='macro', zero_division=0)\n",
        "\n",
        "# Setup cross-validation (5-fold)\n",
        "cv_folds = 5\n",
        "kfold = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "for C in C_values:\n",
        "    print(f\"\\nTraining LogisticRegression with C={C:.2f}...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Create model with LogisticRegression\n",
        "    model = OneVsRestClassifier(\n",
        "        LogisticRegression(\n",
        "            C=C,\n",
        "            penalty='elasticnet',\n",
        "            solver='saga',  # 'saga' supports elasticnet penalty\n",
        "            l1_ratio=0.5,  # Balance between L1 and L2 (0.5 = equal mix)\n",
        "            max_iter=1000,\n",
        "            tol=1e-3,\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Perform cross-validation on training data\n",
        "    print(f\"  Performing {cv_folds}-fold cross-validation...\")\n",
        "    cv_scores = cross_val_score(\n",
        "        model, \n",
        "        X_train_selected, \n",
        "        y_train_binary,\n",
        "        cv=kfold,\n",
        "        scoring=f1_macro_scorer,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    cv_mean = cv_scores.mean()\n",
        "    cv_std = cv_scores.std()\n",
        "    print(f\"  CV F1 Macro: {cv_mean:.4f} (+/- {cv_std:.4f})\")\n",
        "    \n",
        "    # Train on full training set\n",
        "    model.fit(X_train_selected, y_train_binary)\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    metrics = evaluate_model(model, X_train_selected, X_test_selected, y_train_binary, y_test_binary)\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    # Store results\n",
        "    result = {\n",
        "        'C': C,\n",
        "        'cv_f1_macro_mean': cv_mean,\n",
        "        'cv_f1_macro_std': cv_std,\n",
        "        'cv_f1_macro_min': cv_scores.min(),\n",
        "        'cv_f1_macro_max': cv_scores.max(),\n",
        "        'train_f1_macro': metrics['train']['f1_macro'],\n",
        "        'test_f1_macro': metrics['test']['f1_macro'],\n",
        "        'train_f1_micro': metrics['train']['f1_micro'],\n",
        "        'test_f1_micro': metrics['test']['f1_micro'],\n",
        "        'train_hamming_loss': metrics['train']['hamming_loss'],\n",
        "        'test_hamming_loss': metrics['test']['hamming_loss'],\n",
        "        'overfitting_gap_f1_macro': metrics['overfitting_gap']['f1_macro'],\n",
        "        'overfitting_gap_f1_micro': metrics['overfitting_gap']['f1_micro'],\n",
        "        'training_time': training_time\n",
        "    }\n",
        "    results.append(result)\n",
        "    print(f\"  ✓ Completed in {training_time:.2f}s - Test F1 Macro: {metrics['test']['f1_macro']:.4f}\")\n",
        "\n",
        "# Convert to DataFrame for easier analysis\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n✓ C value testing with cross-validation complete!\")\n",
        "print(f\"\\nResults summary:\")\n",
        "display(results_df)\n",
        "\n",
        "# Print best C based on CV score\n",
        "best_cv_idx = results_df['cv_f1_macro_mean'].idxmax()\n",
        "best_cv_C = results_df.loc[best_cv_idx, 'C']\n",
        "print(f\"\\nBest C (highest CV F1 Macro): {best_cv_C:.2f}\")\n",
        "print(f\"  CV F1 Macro: {results_df.loc[best_cv_idx, 'cv_f1_macro_mean']:.4f} (+/- {results_df.loc[best_cv_idx, 'cv_f1_macro_std']:.4f})\")\n",
        "print(f\"  Test F1 Macro: {results_df.loc[best_cv_idx, 'test_f1_macro']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73c58e1e",
      "metadata": {},
      "source": [
        "## 6. Visualize Overfitting Analysis\n",
        "\n",
        "Visualize how different alpha values affect train vs test performance and overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33656aaf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Overfitting Analysis: Alpha Regularization Parameter', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot 1: F1 Macro Score (Train vs Test)\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(results_df['C'], results_df['train_f1_macro'], 'o-', label='Train F1 (macro)', linewidth=2, markersize=8)\n",
        "ax1.plot(results_df['C'], results_df['test_f1_macro'], 's-', label='Test F1 (macro)', linewidth=2, markersize=8)\n",
        "ax1.set_xscale('log')\n",
        "ax1.set_xlabel('C (Regularization Strength, larger = less regularization)', fontsize=12)\n",
        "ax1.set_ylabel('F1 Score (Macro)', fontsize=12)\n",
        "ax1.set_title('F1 Macro Score: Train vs Test', fontsize=13, fontweight='bold')\n",
        "ax1.legend(fontsize=11)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Overfitting Gap (F1 Macro)\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(results_df['C'], results_df['overfitting_gap_f1_macro'], 'ro-', linewidth=2, markersize=8)\n",
        "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
        "ax2.set_xscale('log')\n",
        "ax2.set_xlabel('C (Regularization Strength)', fontsize=12)\n",
        "ax2.set_ylabel('Overfitting Gap (Train - Test)', fontsize=12)\n",
        "ax2.set_title('Overfitting Gap: F1 Macro Score', fontsize=13, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "# Add annotation for best C (smallest gap)\n",
        "best_alpha_idx = results_df['overfitting_gap_f1_macro'].abs().idxmin()\n",
        "best_alpha = results_df.loc[best_alpha_idx, 'C']\n",
        "best_gap = results_df.loc[best_alpha_idx, 'overfitting_gap_f1_macro']\n",
        "ax2.annotate(f'Best C={best_alpha:.2f}\\nGap={best_gap:.4f}', \n",
        "             xy=(best_alpha, best_gap), \n",
        "             xytext=(10, 10), textcoords='offset points',\n",
        "             bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n",
        "             fontsize=10, fontweight='bold')\n",
        "\n",
        "# Plot 3: Hamming Loss (Train vs Test)\n",
        "ax3 = axes[1, 0]\n",
        "ax3.plot(results_df['C'], results_df['train_hamming_loss'], 'o-', label='Train Hamming Loss', linewidth=2, markersize=8)\n",
        "ax3.plot(results_df['C'], results_df['test_hamming_loss'], 's-', label='Test Hamming Loss', linewidth=2, markersize=8)\n",
        "ax3.set_xscale('log')\n",
        "ax3.set_xlabel('C (Regularization Strength)', fontsize=12)\n",
        "ax3.set_ylabel('Hamming Loss', fontsize=12)\n",
        "ax3.set_title('Hamming Loss: Train vs Test (Lower is Better)', fontsize=13, fontweight='bold')\n",
        "ax3.legend(fontsize=11)\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: F1 Micro Score (Train vs Test)\n",
        "ax4 = axes[1, 1]\n",
        "ax4.plot(results_df['C'], results_df['train_f1_micro'], 'o-', label='Train F1 (micro)', linewidth=2, markersize=8)\n",
        "ax4.plot(results_df['C'], results_df['test_f1_micro'], 's-', label='Test F1 (micro)', linewidth=2, markersize=8)\n",
        "ax4.set_xscale('log')\n",
        "ax4.set_xlabel('C (Regularization Strength)', fontsize=12)\n",
        "ax4.set_ylabel('F1 Score (Micro)', fontsize=12)\n",
        "ax4.set_title('F1 Micro Score: Train vs Test', fontsize=13, fontweight='bold')\n",
        "ax4.legend(fontsize=11)\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"OVERFITTING ANALYSIS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nBest C (smallest overfitting gap): {best_alpha:.2f}\")\n",
        "print(f\"  Train F1 (macro): {results_df.loc[best_alpha_idx, 'train_f1_macro']:.4f}\")\n",
        "print(f\"  Test F1 (macro): {results_df.loc[best_alpha_idx, 'test_f1_macro']:.4f}\")\n",
        "print(f\"  Overfitting gap: {best_gap:.4f}\")\n",
        "\n",
        "# Find C with best test performance\n",
        "best_test_idx = results_df['test_f1_macro'].idxmax()\n",
        "best_test_alpha = results_df.loc[best_test_idx, 'C']\n",
        "print(f\"\\nBest C (highest test F1): {best_test_alpha:.2f}\")\n",
        "print(f\"  Train F1 (macro): {results_df.loc[best_test_idx, 'train_f1_macro']:.4f}\")\n",
        "print(f\"  Test F1 (macro): {results_df.loc[best_test_idx, 'test_f1_macro']:.4f}\")\n",
        "print(f\"  Overfitting gap: {results_df.loc[best_test_idx, 'overfitting_gap_f1_macro']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "movie_genre_model",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
