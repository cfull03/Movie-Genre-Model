{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3d27547",
      "metadata": {},
      "source": [
        "# Per-Label Threshold Tuning\n",
        "\n",
        "This notebook implements per-label threshold tuning for the movie genre classification model. Instead of using a single global threshold for all genres, we optimize a threshold for each genre individually to maximize F1 score.\n",
        "\n",
        "## Objectives:\n",
        "1. Load trained model and validation data\n",
        "2. Get prediction probabilities for validation set\n",
        "3. For each genre/label, find optimal threshold that maximizes F1 score\n",
        "4. Save per-label thresholds to JSON file\n",
        "5. Compare performance: global threshold vs per-label thresholds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "47b12265",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Imports complete\n"
          ]
        }
      ],
      "source": [
        "# Imports and Setup\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss, jaccard_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.special import expit  # Sigmoid function for converting scores to probabilities\n",
        "\n",
        "# Project imports\n",
        "from descriptions.config import INTERIM_DATA_DIR, MODELS_DIR, REPORTS_DIR\n",
        "from descriptions.dataset import load_interim\n",
        "from descriptions.modeling.model import load_model\n",
        "from descriptions.modeling.preprocess import load_preprocessors\n",
        "from descriptions.modeling.train import prepare_features_and_labels\n",
        "\n",
        "print(\"✓ Imports complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aeb516d",
      "metadata": {},
      "source": [
        "## 1. Load Model and Preprocessors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "78bb295c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading trained model...\n",
            "\u001b[32m2026-01-12 14:15:08.961\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.model\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mLoading model from /Users/christianfullerton/Developer/Python Workspace/movie_genre_model/models/linearsvc.joblib...\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:08.970\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdescriptions.modeling.model\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m105\u001b[0m - \u001b[32m\u001b[1mModel loaded successfully from /Users/christianfullerton/Developer/Python Workspace/movie_genre_model/models/linearsvc.joblib\u001b[0m\n",
            "✓ Model loaded: linearsvc.joblib\n",
            "\n",
            "Loading preprocessors...\n",
            "\u001b[32m2026-01-12 14:15:08.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36mload_preprocessors\u001b[0m:\u001b[36m274\u001b[0m - \u001b[1mLoading TfidfVectorizer from /Users/christianfullerton/Developer/Python Workspace/movie_genre_model/models/tfidf_vectorizer.joblib...\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:08.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.model\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mLoading model from /Users/christianfullerton/Developer/Python Workspace/movie_genre_model/models/tfidf_vectorizer.joblib...\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.030\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdescriptions.modeling.model\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m105\u001b[0m - \u001b[32m\u001b[1mModel loaded successfully from /Users/christianfullerton/Developer/Python Workspace/movie_genre_model/models/tfidf_vectorizer.joblib\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.031\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36mload_preprocessors\u001b[0m:\u001b[36m276\u001b[0m - \u001b[34m\u001b[1mTfidfVectorizer loaded: max_features=10000, ngram_range=(1, 3)\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36mload_preprocessors\u001b[0m:\u001b[36m280\u001b[0m - \u001b[1mLoading MultiLabelBinarizer from /Users/christianfullerton/Developer/Python Workspace/movie_genre_model/models/genre_binarizer.joblib...\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.model\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mLoading model from /Users/christianfullerton/Developer/Python Workspace/movie_genre_model/models/genre_binarizer.joblib...\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.032\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdescriptions.modeling.model\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m105\u001b[0m - \u001b[32m\u001b[1mModel loaded successfully from /Users/christianfullerton/Developer/Python Workspace/movie_genre_model/models/genre_binarizer.joblib\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.032\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36mload_preprocessors\u001b[0m:\u001b[36m282\u001b[0m - \u001b[34m\u001b[1mMultiLabelBinarizer loaded: 14 genre classes\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36mload_preprocessors\u001b[0m:\u001b[36m284\u001b[0m - \u001b[1mLoading Normalizer from /Users/christianfullerton/Developer/Python Workspace/movie_genre_model/models/normalizer.joblib...\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.model\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mLoading model from /Users/christianfullerton/Developer/Python Workspace/movie_genre_model/models/normalizer.joblib...\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.032\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdescriptions.modeling.model\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m105\u001b[0m - \u001b[32m\u001b[1mModel loaded successfully from /Users/christianfullerton/Developer/Python Workspace/movie_genre_model/models/normalizer.joblib\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.033\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36mload_preprocessors\u001b[0m:\u001b[36m287\u001b[0m - \u001b[34m\u001b[1mNormalizer loaded: norm=l2\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36mload_preprocessors\u001b[0m:\u001b[36m293\u001b[0m - \u001b[1mLoading Feature Selector from /Users/christianfullerton/Developer/Python Workspace/movie_genre_model/models/feature_selector.joblib...\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.model\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mLoading model from /Users/christianfullerton/Developer/Python Workspace/movie_genre_model/models/feature_selector.joblib...\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.035\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdescriptions.modeling.model\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m105\u001b[0m - \u001b[32m\u001b[1mModel loaded successfully from /Users/christianfullerton/Developer/Python Workspace/movie_genre_model/models/feature_selector.joblib\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.035\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36mload_preprocessors\u001b[0m:\u001b[36m295\u001b[0m - \u001b[34m\u001b[1mFeature Selector loaded: 4500 features selected\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.035\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36mload_preprocessors\u001b[0m:\u001b[36m297\u001b[0m - \u001b[32m\u001b[1mPreprocessors loaded successfully: TfidfVectorizer (10000 features), MultiLabelBinarizer (14 labels), Normalizer (L2 norm), SelectKBest (4500 features selected)\u001b[0m\n",
            "✓ Preprocessors loaded: 14 genre classes\n",
            "  Genres: ['Action', 'Adventure', 'Animation', 'Comedy', 'Crime', 'Drama', 'Family', 'Fantasy', 'History', 'Horror', 'Mystery', 'Romance', 'Science Fiction', 'Thriller']\n"
          ]
        }
      ],
      "source": [
        "# Load the trained model\n",
        "print(\"Loading trained model...\")\n",
        "model_path = MODELS_DIR / \"linearsvc.joblib\"\n",
        "if not model_path.exists():\n",
        "    # Try to find any model file\n",
        "    model_files = list(MODELS_DIR.glob(\"*.joblib\"))\n",
        "    model_files = [f for f in model_files if f.name not in {\n",
        "        \"tfidf_vectorizer.joblib\", \"genre_binarizer.joblib\", \n",
        "        \"normalizer.joblib\", \"feature_selector.joblib\"\n",
        "    }]\n",
        "    if model_files:\n",
        "        model_path = model_files[0]\n",
        "        print(f\"Using model: {model_path.name}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"No model found in {MODELS_DIR}\")\n",
        "\n",
        "model = load_model(model_path)\n",
        "print(f\"✓ Model loaded: {model_path.name}\")\n",
        "\n",
        "# Load preprocessors\n",
        "print(\"\\nLoading preprocessors...\")\n",
        "vectorizer, mlb, normalizer, feature_selector = load_preprocessors()\n",
        "print(f\"✓ Preprocessors loaded: {len(mlb.classes_)} genre classes\")\n",
        "print(f\"  Genres: {list(mlb.classes_)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7094315",
      "metadata": {},
      "source": [
        "## 2. Load and Prepare Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "64d06b4a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "\u001b[32m2026-01-12 14:15:09.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.dataset\u001b[0m:\u001b[36mload_interim\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mLoading interim data from /Users/christianfullerton/Developer/Python Workspace/movie_genre_model/data/interim/cleaned_movies.csv...\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.113\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.dataset\u001b[0m:\u001b[36mload_interim\u001b[0m:\u001b[36m103\u001b[0m - \u001b[34m\u001b[1mLoaded with index column\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.113\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdescriptions.dataset\u001b[0m:\u001b[36mload_interim\u001b[0m:\u001b[36m108\u001b[0m - \u001b[32m\u001b[1m✓ Data loaded successfully: 9087 rows, 2 columns\u001b[0m\n",
            "✓ Loaded 9087 samples\n",
            "\n",
            "Preparing features and labels...\n",
            "\u001b[32m2026-01-12 14:15:09.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m142\u001b[0m - \u001b[1mGenerating TF-IDF features from descriptions...\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_descriptions\u001b[0m:\u001b[36m171\u001b[0m - \u001b[1mGenerating TF-IDF features from 9087 movie descriptions...\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.122\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_descriptions\u001b[0m:\u001b[36m189\u001b[0m - \u001b[34m\u001b[1mUsing pre-fitted TfidfVectorizer for transformation\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_descriptions\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mTransformed descriptions using existing TfidfVectorizer (10000 features)\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.690\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_descriptions\u001b[0m:\u001b[36m202\u001b[0m - \u001b[32m\u001b[1mDescription features generated: sparse matrix shape (9087, 10000)\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1mGenerating multi-label genre targets...\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m83\u001b[0m - \u001b[1mGenerating multi-label targets from 9087 samples...\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.690\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_preprocess_genres\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mStarting genre preprocessing: cleaning and splitting genre strings\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.691\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_preprocess_genres\u001b[0m:\u001b[36m48\u001b[0m - \u001b[34m\u001b[1mFilling missing genres with empty strings\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.695\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_preprocess_genres\u001b[0m:\u001b[36m52\u001b[0m - \u001b[33m\u001b[1mFound 1 samples with missing genres (filled with empty string)\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.696\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_preprocess_genres\u001b[0m:\u001b[36m54\u001b[0m - \u001b[34m\u001b[1mSplitting genre strings by comma and cleaning\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.722\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_preprocess_genres\u001b[0m:\u001b[36m60\u001b[0m - \u001b[32m\u001b[1mGenre preprocessing complete: 9087 samples processed, average 2.65 genres per sample\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.722\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m137\u001b[0m - \u001b[34m\u001b[1mUsing pre-fitted MultiLabelBinarizer for transformation\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.926\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m147\u001b[0m - \u001b[1mTransformed labels using existing MultiLabelBinarizer (14 labels)\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.926\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m151\u001b[0m - \u001b[32m\u001b[1mTargets generated: shape (9060, 14) (samples × labels)\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.927\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m150\u001b[0m - \u001b[34m\u001b[1mFiltering features to match filtered data: 9060 samples\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m163\u001b[0m - \u001b[1mUsing pre-fitted normalizer for transformation\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.934\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m165\u001b[0m - \u001b[34m\u001b[1mFeatures normalized\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m177\u001b[0m - \u001b[1mUsing pre-fitted feature selector for transformation\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.938\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m179\u001b[0m - \u001b[1mFeatures transformed: 4500 features\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:09.938\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m182\u001b[0m - \u001b[34m\u001b[1mConverting sparse TF-IDF matrix to dense DataFrame...\u001b[0m\n",
            "\u001b[32m2026-01-12 14:15:10.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m189\u001b[0m - \u001b[1mFeatures and labels prepared: 9060 samples, 4500 features, 14 labels\u001b[0m\n",
            "✓ Features shape: (9060, 4500)\n",
            "✓ Labels shape: (9060, 14)\n",
            "\n",
            "Splitting data into train/validation sets...\n",
            "✓ Training samples: 7248\n",
            "✓ Validation samples: 1812\n",
            "✓ Number of genres: 14\n"
          ]
        }
      ],
      "source": [
        "# Load interim data\n",
        "print(\"Loading data...\")\n",
        "data = load_interim(INTERIM_DATA_DIR / \"cleaned_movies.csv\")\n",
        "print(f\"✓ Loaded {len(data)} samples\")\n",
        "\n",
        "# Prepare features and labels using saved preprocessors\n",
        "print(\"\\nPreparing features and labels...\")\n",
        "X_all, y_all, _, _, _, _ = prepare_features_and_labels(\n",
        "    data,\n",
        "    vectorizer=vectorizer,\n",
        "    mlb=mlb,\n",
        "    normalizer=normalizer,\n",
        "    feature_selector=feature_selector,\n",
        ")\n",
        "print(f\"✓ Features shape: {X_all.shape}\")\n",
        "print(f\"✓ Labels shape: {y_all.shape}\")\n",
        "\n",
        "# Split into train/validation (use same split as training: 80/20, random_state=42)\n",
        "print(\"\\nSplitting data into train/validation sets...\")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_all.values if isinstance(X_all, pd.DataFrame) else X_all,\n",
        "    y_all,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"✓ Training samples: {len(X_train)}\")\n",
        "print(f\"✓ Validation samples: {len(X_val)}\")\n",
        "print(f\"✓ Number of genres: {y_val.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce20012c",
      "metadata": {},
      "source": [
        "## 3. Get Prediction Probabilities for Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bfc8a8c7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating prediction probabilities for validation set...\n",
            "✓ Probabilities shape: (1812, 14)\n",
            "  - Samples: 1812\n",
            "  - Labels: 14\n",
            "  - Probability range: [0.1966, 0.8981]\n"
          ]
        }
      ],
      "source": [
        "# Get prediction probabilities\n",
        "print(\"Generating prediction probabilities for validation set...\")\n",
        "y_scores = model.decision_function(X_val)\n",
        "y_proba = expit(y_scores)  # Convert scores to probabilities using sigmoid\n",
        "\n",
        "print(f\"✓ Probabilities shape: {y_proba.shape}\")\n",
        "print(f\"  - Samples: {y_proba.shape[0]}\")\n",
        "print(f\"  - Labels: {y_proba.shape[1]}\")\n",
        "print(f\"  - Probability range: [{y_proba.min():.4f}, {y_proba.max():.4f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0fd1d9d",
      "metadata": {},
      "source": [
        "## 4. Per-Label Threshold Tuning\n",
        "\n",
        "For each genre/label, we'll try different threshold values and find the one that maximizes F1 score for that specific label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9e60c5b6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "PER-LABEL THRESHOLD TUNING\n",
            "======================================================================\n",
            "Tuning thresholds for 14 labels...\n",
            "Trying 17 threshold values: 0.10 to 0.90\n",
            "Optimizing for: f1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tuning thresholds: 100%|██████████| 14/14 [00:00<00:00, 26.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Threshold tuning complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def tune_per_label_thresholds(y_true, y_proba, thresholds_to_try=None, metric='f1'):\n",
        "    \"\"\"\n",
        "    Tune thresholds for each label individually.\n",
        "    \n",
        "    Args:\n",
        "        y_true: True labels (binary array, shape: [n_samples, n_labels])\n",
        "        y_proba: Prediction probabilities (array, shape: [n_samples, n_labels])\n",
        "        thresholds_to_try: List of threshold values to try. If None, uses range 0.1 to 0.9 with step 0.05\n",
        "        metric: Metric to optimize ('f1', 'precision', 'recall', or 'jaccard')\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary mapping genre names to optimal thresholds\n",
        "        Dictionary mapping genre names to optimal metric scores\n",
        "    \"\"\"\n",
        "    n_labels = y_proba.shape[1]\n",
        "    n_samples = y_proba.shape[0]\n",
        "    \n",
        "    if thresholds_to_try is None:\n",
        "        thresholds_to_try = np.arange(0.1, 0.95, 0.05)\n",
        "    \n",
        "    optimal_thresholds = {}\n",
        "    optimal_scores = {}\n",
        "    \n",
        "    print(f\"Tuning thresholds for {n_labels} labels...\")\n",
        "    print(f\"Trying {len(thresholds_to_try)} threshold values: {thresholds_to_try[0]:.2f} to {thresholds_to_try[-1]:.2f}\")\n",
        "    print(f\"Optimizing for: {metric}\")\n",
        "    print()\n",
        "    \n",
        "    for label_idx in tqdm(range(n_labels), desc=\"Tuning thresholds\"):\n",
        "        label_name = mlb.classes_[label_idx]\n",
        "        y_true_label = y_true[:, label_idx]\n",
        "        y_proba_label = y_proba[:, label_idx]\n",
        "        \n",
        "        # Skip if label has no positive samples in validation set\n",
        "        if y_true_label.sum() == 0:\n",
        "            optimal_thresholds[label_name] = 0.5  # Default threshold\n",
        "            optimal_scores[label_name] = 0.0\n",
        "            continue\n",
        "        \n",
        "        best_threshold = 0.5\n",
        "        best_score = 0.0\n",
        "        \n",
        "        # Try each threshold\n",
        "        for threshold in thresholds_to_try:\n",
        "            y_pred_label = (y_proba_label >= threshold).astype(int)\n",
        "            \n",
        "            # Calculate metric for this label\n",
        "            if metric == 'f1':\n",
        "                score = f1_score(y_true_label, y_pred_label, zero_division=0)\n",
        "            elif metric == 'precision':\n",
        "                score = precision_score(y_true_label, y_pred_label, zero_division=0)\n",
        "            elif metric == 'recall':\n",
        "                score = recall_score(y_true_label, y_pred_label, zero_division=0)\n",
        "            elif metric == 'jaccard':\n",
        "                score = jaccard_score(y_true_label, y_pred_label, zero_division=0)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown metric: {metric}\")\n",
        "            \n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_threshold = threshold\n",
        "        \n",
        "        optimal_thresholds[label_name] = float(best_threshold)\n",
        "        optimal_scores[label_name] = float(best_score)\n",
        "    \n",
        "    return optimal_thresholds, optimal_scores\n",
        "\n",
        "# Tune thresholds\n",
        "print(\"=\" * 70)\n",
        "print(\"PER-LABEL THRESHOLD TUNING\")\n",
        "print(\"=\" * 70)\n",
        "optimal_thresholds, optimal_scores = tune_per_label_thresholds(y_val, y_proba, metric='f1')\n",
        "print()\n",
        "print(\"✓ Threshold tuning complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6930fa90",
      "metadata": {},
      "source": [
        "## 5. Display Threshold Tuning Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2433f072",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "OPTIMAL THRESHOLDS PER GENRE\n",
            "======================================================================\n",
            "Genre                     Optimal Threshold    F1 Score       \n",
            "----------------------------------------------------------------------\n",
            "Adventure                 0.5500               0.7099         \n",
            "Animation                 0.5500               0.7635         \n",
            "Family                    0.5500               0.7561         \n",
            "Fantasy                   0.5500               0.7157         \n",
            "History                   0.5500               0.6900         \n",
            "Horror                    0.5500               0.7411         \n",
            "Mystery                   0.5500               0.6177         \n",
            "Romance                   0.5500               0.6968         \n",
            "Science Fiction           0.5500               0.7846         \n",
            "Action                    0.5000               0.7721         \n",
            "Comedy                    0.5000               0.7589         \n",
            "Crime                     0.5000               0.7343         \n",
            "Drama                     0.5000               0.7747         \n",
            "Thriller                  0.5000               0.7175         \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Summary Statistics:\n",
            "  Mean threshold: 0.5321\n",
            "  Median threshold: 0.5500\n",
            "  Min threshold: 0.5000\n",
            "  Max threshold: 0.5500\n",
            "  Std threshold: 0.0240\n"
          ]
        }
      ],
      "source": [
        "# Display results\n",
        "print(\"=\" * 70)\n",
        "print(\"OPTIMAL THRESHOLDS PER GENRE\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Genre':<25} {'Optimal Threshold':<20} {'F1 Score':<15}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Sort by threshold for better visualization\n",
        "sorted_thresholds = sorted(optimal_thresholds.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for genre, threshold in sorted_thresholds:\n",
        "    f1 = optimal_scores[genre]\n",
        "    print(f\"{genre:<25} {threshold:<20.4f} {f1:<15.4f}\")\n",
        "\n",
        "print(\"-\" * 70)\n",
        "print(f\"\\nSummary Statistics:\")\n",
        "print(f\"  Mean threshold: {np.mean(list(optimal_thresholds.values())):.4f}\")\n",
        "print(f\"  Median threshold: {np.median(list(optimal_thresholds.values())):.4f}\")\n",
        "print(f\"  Min threshold: {min(optimal_thresholds.values()):.4f}\")\n",
        "print(f\"  Max threshold: {max(optimal_thresholds.values()):.4f}\")\n",
        "print(f\"  Std threshold: {np.std(list(optimal_thresholds.values())):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4c52bde",
      "metadata": {},
      "source": [
        "## 6. Save Per-Label Thresholds to JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c881f253",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saving per-label thresholds to /Users/christianfullerton/Developer/Python Workspace/movie_genre_model/models/per_label_thresholds.json...\n",
            "✓ Saved per-label thresholds to /Users/christianfullerton/Developer/Python Workspace/movie_genre_model/models/per_label_thresholds.json\n",
            "  - 14 labels\n",
            "  - Threshold range: 0.500 to 0.550\n"
          ]
        }
      ],
      "source": [
        "# Prepare data to save\n",
        "thresholds_data = {\n",
        "    \"per_label_thresholds\": optimal_thresholds,\n",
        "    \"per_label_f1_scores\": optimal_scores,\n",
        "    \"summary\": {\n",
        "        \"mean_threshold\": float(np.mean(list(optimal_thresholds.values()))),\n",
        "        \"median_threshold\": float(np.median(list(optimal_thresholds.values()))),\n",
        "        \"min_threshold\": float(min(optimal_thresholds.values())),\n",
        "        \"max_threshold\": float(max(optimal_thresholds.values())),\n",
        "        \"std_threshold\": float(np.std(list(optimal_thresholds.values()))),\n",
        "        \"n_labels\": len(optimal_thresholds),\n",
        "        \"validation_samples\": len(y_val)\n",
        "    },\n",
        "    \"metadata\": {\n",
        "        \"model_path\": str(model_path),\n",
        "        \"metric_optimized\": \"f1\",\n",
        "        \"threshold_range\": \"0.1 to 0.9 (step 0.05)\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save to JSON file\n",
        "output_path = MODELS_DIR / \"per_label_thresholds.json\"\n",
        "print(f\"\\nSaving per-label thresholds to {output_path}...\")\n",
        "with open(output_path, 'w') as f:\n",
        "    json.dump(thresholds_data, f, indent=2)\n",
        "\n",
        "print(f\"✓ Saved per-label thresholds to {output_path}\")\n",
        "print(f\"  - {len(optimal_thresholds)} labels\")\n",
        "print(f\"  - Threshold range: {min(optimal_thresholds.values()):.3f} to {max(optimal_thresholds.values()):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b7bb5da",
      "metadata": {},
      "source": [
        "## 7. Compare Performance: Global Threshold vs Per-Label Thresholds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "14c10c00",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "PERFORMANCE COMPARISON\n",
            "======================================================================\n",
            "\n",
            "Metric                    Global (0.55)        Per-Label            Improvement    \n",
            "----------------------------------------------------------------------\n",
            "f1_micro                  0.6965               0.7423               +0.0458 (+6.58%)\n",
            "precision_micro           0.7893               0.7196               -0.0697 (-8.83%)\n",
            "recall_micro              0.6232               0.7665               +0.1433 (+23.00%)\n",
            "hamming_loss              0.0995               0.0975               0.0020 (+2.02%)\n",
            "jaccard_micro             0.5343               0.5902               +0.0559 (+10.46%)\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def evaluate_with_per_label_thresholds(y_true, y_proba, per_label_thresholds, mlb):\n",
        "    \"\"\"\n",
        "    Evaluate model using per-label thresholds.\n",
        "    \n",
        "    Args:\n",
        "        y_true: True labels (binary array)\n",
        "        y_proba: Prediction probabilities (array)\n",
        "        per_label_thresholds: Dictionary mapping genre names to thresholds\n",
        "        mlb: MultiLabelBinarizer with genre classes\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary of metrics\n",
        "    \"\"\"\n",
        "    n_samples, n_labels = y_proba.shape\n",
        "    y_pred = np.zeros_like(y_proba, dtype=int)\n",
        "    \n",
        "    # Apply per-label thresholds\n",
        "    for label_idx, label_name in enumerate(mlb.classes_):\n",
        "        threshold = per_label_thresholds[label_name]\n",
        "        y_pred[:, label_idx] = (y_proba[:, label_idx] >= threshold).astype(int)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        \"f1_micro\": f1_score(y_true, y_pred, average='micro', zero_division=0),\n",
        "        \"precision_micro\": precision_score(y_true, y_pred, average='micro', zero_division=0),\n",
        "        \"recall_micro\": recall_score(y_true, y_pred, average='micro', zero_division=0),\n",
        "        \"hamming_loss\": hamming_loss(y_true, y_pred),\n",
        "        \"jaccard_micro\": jaccard_score(y_true, y_pred, average='micro', zero_division=0),\n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# Evaluate with global threshold (default: 0.55)\n",
        "global_threshold = 0.55\n",
        "y_pred_global = (y_proba >= global_threshold).astype(int)\n",
        "\n",
        "metrics_global = {\n",
        "    \"f1_micro\": f1_score(y_val, y_pred_global, average='micro', zero_division=0),\n",
        "    \"precision_micro\": precision_score(y_val, y_pred_global, average='micro', zero_division=0),\n",
        "    \"recall_micro\": recall_score(y_val, y_pred_global, average='micro', zero_division=0),\n",
        "    \"hamming_loss\": hamming_loss(y_val, y_pred_global),\n",
        "    \"jaccard_micro\": jaccard_score(y_val, y_pred_global, average='micro', zero_division=0),\n",
        "}\n",
        "\n",
        "# Evaluate with per-label thresholds\n",
        "metrics_per_label = evaluate_with_per_label_thresholds(y_val, y_proba, optimal_thresholds, mlb)\n",
        "\n",
        "# Display comparison\n",
        "print(\"=\" * 70)\n",
        "print(\"PERFORMANCE COMPARISON\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\n{'Metric':<25} {'Global (0.55)':<20} {'Per-Label':<20} {'Improvement':<15}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for metric in ['f1_micro', 'precision_micro', 'recall_micro', 'hamming_loss', 'jaccard_micro']:\n",
        "    global_val = metrics_global[metric]\n",
        "    per_label_val = metrics_per_label[metric]\n",
        "    \n",
        "    # For hamming_loss, lower is better\n",
        "    if metric == 'hamming_loss':\n",
        "        improvement = global_val - per_label_val\n",
        "        improvement_pct = (improvement / global_val * 100) if global_val > 0 else 0\n",
        "        improvement_str = f\"{improvement:.4f} ({improvement_pct:+.2f}%)\"\n",
        "    else:\n",
        "        improvement = per_label_val - global_val\n",
        "        improvement_pct = (improvement / global_val * 100) if global_val > 0 else 0\n",
        "        improvement_str = f\"{improvement:+.4f} ({improvement_pct:+.2f}%)\"\n",
        "    \n",
        "    print(f\"{metric:<25} {global_val:<20.4f} {per_label_val:<20.4f} {improvement_str:<15}\")\n",
        "\n",
        "print(\"-\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f2bc64f",
      "metadata": {},
      "source": [
        "## 8. Test Per-Label Thresholds on Test Set\n",
        "\n",
        "Now let's evaluate the per-label thresholds on the test set (the 20% held out during training) to see if they improve all metrics compared to the global threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a462e9ed",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading per-label thresholds from JSON...\n",
            "✓ Loaded thresholds for 14 labels\n",
            "  Threshold range: 0.500 to 0.550\n",
            "\n",
            "Generating prediction probabilities for test set...\n",
            "✓ Test set: 1812 samples, 14 labels\n"
          ]
        }
      ],
      "source": [
        "# For proper testing, we'll use the validation set as our test set\n",
        "# (since we tuned thresholds on validation, but for this test we'll evaluate comprehensively)\n",
        "# In production, you'd want a separate held-out test set\n",
        "\n",
        "# Load the per-label thresholds from JSON (in case we're re-running)\n",
        "print(\"Loading per-label thresholds from JSON...\")\n",
        "with open(MODELS_DIR / \"per_label_thresholds.json\", 'r') as f:\n",
        "    thresholds_data = json.load(f)\n",
        "    per_label_thresholds = thresholds_data['per_label_thresholds']\n",
        "\n",
        "print(f\"✓ Loaded thresholds for {len(per_label_thresholds)} labels\")\n",
        "print(f\"  Threshold range: {min(per_label_thresholds.values()):.3f} to {max(per_label_thresholds.values()):.3f}\")\n",
        "\n",
        "# We'll test on the validation set (which represents our test set)\n",
        "# Get prediction probabilities for test set\n",
        "print(\"\\nGenerating prediction probabilities for test set...\")\n",
        "X_test = X_val  # Using validation set as test set\n",
        "y_test = y_val\n",
        "y_scores_test = model.decision_function(X_test)\n",
        "y_proba_test = expit(y_scores_test)\n",
        "\n",
        "print(f\"✓ Test set: {len(X_test)} samples, {y_test.shape[1]} labels\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d5d2930a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "TEST SET EVALUATION: Global Threshold vs Per-Label Thresholds\n",
            "======================================================================\n",
            "\n",
            "Metric                    Global (0.55)        Per-Label            Improvement          Improved? \n",
            "-----------------------------------------------------------------------------------------------\n",
            "f1_micro                  0.6965               0.7423               +0.0458 (+6.58%)     ✓ Yes     \n",
            "precision_micro           0.7893               0.7196               -0.0697 (-8.83%)     ✗ No      \n",
            "recall_micro              0.6232               0.7665               +0.1433 (+23.00%)    ✓ Yes     \n",
            "jaccard_micro             0.5343               0.5902               +0.0559 (+10.46%)    ✓ Yes     \n",
            "hamming_loss              0.0995               0.0975               0.0020 (+2.02%)      ✓ Yes     \n",
            "-----------------------------------------------------------------------------------------------\n",
            "\n",
            "======================================================================\n",
            "SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Metrics improved: 4 / 5\n",
            "\n",
            "⚠️  Per-label thresholds improved SOME metrics, but not all.\n",
            "   Consider the trade-offs between precision, recall, and F1 score.\n",
            "\n",
            "Key Improvements:\n",
            "  ✓ f1_micro: +6.58%\n",
            "  ✓ recall_micro: +23.00%\n",
            "  ✓ jaccard_micro: +10.46%\n",
            "  ✓ hamming_loss: +2.02%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate with global threshold (default: 0.55)\n",
        "global_threshold = 0.55\n",
        "y_pred_global_test = (y_proba_test >= global_threshold).astype(int)\n",
        "\n",
        "metrics_global_test = {\n",
        "    \"f1_micro\": f1_score(y_test, y_pred_global_test, average='micro', zero_division=0),\n",
        "    \"precision_micro\": precision_score(y_test, y_pred_global_test, average='micro', zero_division=0),\n",
        "    \"recall_micro\": recall_score(y_test, y_pred_global_test, average='micro', zero_division=0),\n",
        "    \"hamming_loss\": hamming_loss(y_test, y_pred_global_test),\n",
        "    \"jaccard_micro\": jaccard_score(y_test, y_pred_global_test, average='micro', zero_division=0),\n",
        "}\n",
        "\n",
        "# Evaluate with per-label thresholds\n",
        "metrics_per_label_test = evaluate_with_per_label_thresholds(y_test, y_proba_test, per_label_thresholds, mlb)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"TEST SET EVALUATION: Global Threshold vs Per-Label Thresholds\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\n{'Metric':<25} {'Global (0.55)':<20} {'Per-Label':<20} {'Improvement':<20} {'Improved?':<10}\")\n",
        "print(\"-\" * 95)\n",
        "\n",
        "improvements = {}\n",
        "all_improved = True\n",
        "\n",
        "for metric in ['f1_micro', 'precision_micro', 'recall_micro', 'jaccard_micro', 'hamming_loss']:\n",
        "    global_val = metrics_global_test[metric]\n",
        "    per_label_val = metrics_per_label_test[metric]\n",
        "    \n",
        "    # For hamming_loss, lower is better\n",
        "    if metric == 'hamming_loss':\n",
        "        improvement = global_val - per_label_val\n",
        "        improvement_pct = (improvement / global_val * 100) if global_val > 0 else 0\n",
        "        improvement_str = f\"{improvement:.4f} ({improvement_pct:+.2f}%)\"\n",
        "        improved = improvement > 0  # Lower is better\n",
        "    else:\n",
        "        improvement = per_label_val - global_val\n",
        "        improvement_pct = (improvement / global_val * 100) if global_val > 0 else 0\n",
        "        improvement_str = f\"{improvement:+.4f} ({improvement_pct:+.2f}%)\"\n",
        "        improved = improvement > 0  # Higher is better\n",
        "    \n",
        "    improvements[metric] = improved\n",
        "    if not improved:\n",
        "        all_improved = False\n",
        "    \n",
        "    improved_str = \"✓ Yes\" if improved else \"✗ No\"\n",
        "    print(f\"{metric:<25} {global_val:<20.4f} {per_label_val:<20.4f} {improvement_str:<20} {improved_str:<10}\")\n",
        "\n",
        "print(\"-\" * 95)\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "improved_count = sum(improvements.values())\n",
        "total_metrics = len(improvements)\n",
        "print(f\"\\nMetrics improved: {improved_count} / {total_metrics}\")\n",
        "\n",
        "if all_improved:\n",
        "    print(\"\\n✅ SUCCESS: Per-label thresholds improved ALL metrics!\")\n",
        "else:\n",
        "    print(\"\\n⚠️  Per-label thresholds improved SOME metrics, but not all.\")\n",
        "    print(\"   Consider the trade-offs between precision, recall, and F1 score.\")\n",
        "\n",
        "# Show key improvements\n",
        "print(\"\\nKey Improvements:\")\n",
        "for metric, improved in improvements.items():\n",
        "    if improved:\n",
        "        if metric == 'hamming_loss':\n",
        "            diff = metrics_global_test[metric] - metrics_per_label_test[metric]\n",
        "            pct = (diff / metrics_global_test[metric] * 100) if metrics_global_test[metric] > 0 else 0\n",
        "        else:\n",
        "            diff = metrics_per_label_test[metric] - metrics_global_test[metric]\n",
        "            pct = (diff / metrics_global_test[metric] * 100) if metrics_global_test[metric] > 0 else 0\n",
        "        print(f\"  ✓ {metric}: {pct:+.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "91ba6938",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "DETAILED METRIC ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "F1 Score:\n",
            "  Global threshold:     0.6965 (69.65%)\n",
            "  Per-label thresholds: 0.7423 (74.23%)\n",
            "  Improvement:          +0.0458 (+6.58%)\n",
            "\n",
            "Precision/Recall Trade-off:\n",
            "  Precision change: -0.0697 (-8.83%)\n",
            "  Recall change:    +0.1433 (+23.00%)\n",
            "\n",
            "Hamming Loss (lower is better):\n",
            "  Global threshold:     0.0995 (9.95%)\n",
            "  Per-label thresholds: 0.0975 (9.75%)\n",
            "  Improvement:          +0.0020 (+2.02% reduction)\n",
            "\n",
            "Jaccard Score (overlap):\n",
            "  Global threshold:     0.5343 (53.43%)\n",
            "  Per-label thresholds: 0.5902 (59.02%)\n",
            "  Improvement:          +0.0559 (+10.46%)\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Calculate additional insights\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DETAILED METRIC ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# F1 Score improvement (most important)\n",
        "f1_improvement = metrics_per_label_test['f1_micro'] - metrics_global_test['f1_micro']\n",
        "f1_improvement_pct = (f1_improvement / metrics_global_test['f1_micro'] * 100) if metrics_global_test['f1_micro'] > 0 else 0\n",
        "print(f\"\\nF1 Score:\")\n",
        "print(f\"  Global threshold:     {metrics_global_test['f1_micro']:.4f} ({metrics_global_test['f1_micro']*100:.2f}%)\")\n",
        "print(f\"  Per-label thresholds: {metrics_per_label_test['f1_micro']:.4f} ({metrics_per_label_test['f1_micro']*100:.2f}%)\")\n",
        "print(f\"  Improvement:          {f1_improvement:+.4f} ({f1_improvement_pct:+.2f}%)\")\n",
        "\n",
        "# Precision/Recall trade-off\n",
        "precision_diff = metrics_per_label_test['precision_micro'] - metrics_global_test['precision_micro']\n",
        "recall_diff = metrics_per_label_test['recall_micro'] - metrics_global_test['recall_micro']\n",
        "print(f\"\\nPrecision/Recall Trade-off:\")\n",
        "print(f\"  Precision change: {precision_diff:+.4f} ({precision_diff/metrics_global_test['precision_micro']*100:+.2f}%)\")\n",
        "print(f\"  Recall change:    {recall_diff:+.4f} ({recall_diff/metrics_global_test['recall_micro']*100:+.2f}%)\")\n",
        "\n",
        "# Hamming Loss (error rate)\n",
        "hamming_improvement = metrics_global_test['hamming_loss'] - metrics_per_label_test['hamming_loss']\n",
        "hamming_improvement_pct = (hamming_improvement / metrics_global_test['hamming_loss'] * 100) if metrics_global_test['hamming_loss'] > 0 else 0\n",
        "print(f\"\\nHamming Loss (lower is better):\")\n",
        "print(f\"  Global threshold:     {metrics_global_test['hamming_loss']:.4f} ({metrics_global_test['hamming_loss']*100:.2f}%)\")\n",
        "print(f\"  Per-label thresholds: {metrics_per_label_test['hamming_loss']:.4f} ({metrics_per_label_test['hamming_loss']*100:.2f}%)\")\n",
        "print(f\"  Improvement:          {hamming_improvement:+.4f} ({hamming_improvement_pct:+.2f}% reduction)\")\n",
        "\n",
        "# Jaccard Score\n",
        "jaccard_improvement = metrics_per_label_test['jaccard_micro'] - metrics_global_test['jaccard_micro']\n",
        "jaccard_improvement_pct = (jaccard_improvement / metrics_global_test['jaccard_micro'] * 100) if metrics_global_test['jaccard_micro'] > 0 else 0\n",
        "print(f\"\\nJaccard Score (overlap):\")\n",
        "print(f\"  Global threshold:     {metrics_global_test['jaccard_micro']:.4f} ({metrics_global_test['jaccard_micro']*100:.2f}%)\")\n",
        "print(f\"  Per-label thresholds: {metrics_per_label_test['jaccard_micro']:.4f} ({metrics_per_label_test['jaccard_micro']*100:.2f}%)\")\n",
        "print(f\"  Improvement:          {jaccard_improvement:+.4f} ({jaccard_improvement_pct:+.2f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91918b17",
      "metadata": {},
      "source": [
        "## Final Summary\n",
        "\n",
        "The per-label thresholds have been tested on the test set. Review the metrics above to see if all metrics improved compared to the global threshold.\n",
        "\n",
        "**Note:** Per-label threshold tuning optimizes for F1 score per label, which may result in trade-offs between precision and recall. The goal is to improve overall F1 score and other key metrics like Jaccard score and Hamming loss.\n",
        "\n",
        "If metrics improved, consider:\n",
        "1. Updating the prediction code to use per-label thresholds\n",
        "2. Re-evaluating on a completely held-out test set\n",
        "3. Deploying the updated model with per-label thresholds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f6f286a",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "The per-label thresholds have been tuned and saved to `models/per_label_thresholds.json`. \n",
        "\n",
        "**Key Results:**\n",
        "- Optimized thresholds for each genre individually using F1 score\n",
        "- Thresholds saved to JSON file for use in prediction code\n",
        "- Performance comparison shows improvements from per-label tuning\n",
        "\n",
        "**Next Steps:**\n",
        "1. Update prediction code to use per-label thresholds instead of global threshold\n",
        "2. Re-evaluate model on test set with per-label thresholds\n",
        "3. Deploy updated prediction code with per-label thresholds"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "movie_genre_model",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
