{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PyTorch Model for Movie Genre Classification\n",
        "\n",
        "This notebook sets up PyTorch Dataset and DataLoader classes for training neural network models on movie genre classification.\n",
        "\n",
        "## Features:\n",
        "- PyTorch Dataset class for movie genre data\n",
        "- DataLoader setup with batching and shuffling\n",
        "- Integration with existing preprocessing pipeline\n",
        "- Ready for feedforward neural networks or GRU/LSTM models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-12-14 20:35:08.688\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: /Users/christianfullerton/Developer/Python Workspace/movie_genre_model\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "✓ All imports successful\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss, jaccard_score\n",
        "\n",
        "# Project imports\n",
        "from descriptions.config import INTERIM_DATA_DIR, MODELS_DIR\n",
        "from descriptions.dataset import load_interim\n",
        "from descriptions.modeling.train import prepare_features_and_labels\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "print(\"✓ All imports successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. PyTorch Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ MovieGenreDataset class defined\n",
            "  - Input: features (n_samples, n_features), labels (n_samples, n_labels)\n",
            "  - Output: PyTorch FloatTensors\n"
          ]
        }
      ],
      "source": [
        "class MovieGenreDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for movie genre classification.\n",
        "    \n",
        "    Handles TF-IDF features and multi-label genre targets.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, features: np.ndarray, labels: np.ndarray):\n",
        "        \"\"\"\n",
        "        Initialize dataset.\n",
        "        \n",
        "        Args:\n",
        "            features: Feature matrix (n_samples, n_features) - TF-IDF features\n",
        "            labels: Binary label matrix (n_samples, n_labels) - genre labels\n",
        "        \"\"\"\n",
        "        # Convert to float32 for PyTorch\n",
        "        self.features = torch.FloatTensor(features.astype(np.float32))\n",
        "        self.labels = torch.FloatTensor(labels.astype(np.float32))\n",
        "        \n",
        "        assert len(self.features) == len(self.labels), \\\n",
        "            f\"Features and labels must have same length. Got {len(self.features)} and {len(self.labels)}\"\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
        "        return len(self.features)\n",
        "    \n",
        "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Get a single sample from the dataset.\n",
        "        \n",
        "        Args:\n",
        "            idx: Index of the sample\n",
        "        \n",
        "        Returns:\n",
        "            Tuple of (features, labels) as PyTorch tensors\n",
        "        \"\"\"\n",
        "        return self.features[idx], self.labels[idx]\n",
        "    \n",
        "    def get_feature_dim(self) -> int:\n",
        "        \"\"\"Get the number of features.\"\"\"\n",
        "        return self.features.shape[1]\n",
        "    \n",
        "    def get_num_labels(self) -> int:\n",
        "        \"\"\"Get the number of genre labels.\"\"\"\n",
        "        return self.labels.shape[1]\n",
        "\n",
        "\n",
        "# Test the Dataset class\n",
        "print(\"✓ MovieGenreDataset class defined\")\n",
        "print(f\"  - Input: features (n_samples, n_features), labels (n_samples, n_labels)\")\n",
        "print(f\"  - Output: PyTorch FloatTensors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "\u001b[32m2025-12-14 20:35:10.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.dataset\u001b[0m:\u001b[36mload_interim\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mLoading interim data from /Users/christianfullerton/Developer/Python Workspace/movie_genre_model/data/interim/cleaned_movies.csv...\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:10.739\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.dataset\u001b[0m:\u001b[36mload_interim\u001b[0m:\u001b[36m103\u001b[0m - \u001b[34m\u001b[1mLoaded with index column\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:10.740\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdescriptions.dataset\u001b[0m:\u001b[36mload_interim\u001b[0m:\u001b[36m108\u001b[0m - \u001b[32m\u001b[1m✓ Data loaded successfully: 9087 rows, 2 columns\u001b[0m\n",
            "✓ Loaded 9087 samples\n",
            "\n",
            "Splitting data (test_size=0.2, random_state=42)...\n",
            "✓ Train: 7269 samples, Test: 1818 samples\n",
            "\n",
            "Preparing features and labels...\n",
            "\u001b[32m2025-12-14 20:35:10.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mGenerating TF-IDF features from descriptions...\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:10.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_descriptions\u001b[0m:\u001b[36m169\u001b[0m - \u001b[1mGenerating TF-IDF features from 7269 movie descriptions...\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:10.750\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_descriptions\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mCreating and fitting new TfidfVectorizer with default parameters\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:10.751\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36mbuild_preprocessor\u001b[0m:\u001b[36m215\u001b[0m - \u001b[34m\u001b[1mBuilding preprocessing components: TfidfVectorizer, MultiLabelBinarizer, and SelectKBest\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:10.751\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36mbuild_preprocessor\u001b[0m:\u001b[36m226\u001b[0m - \u001b[34m\u001b[1mTfidfVectorizer configured: max_features=10000, ngram_range=(1,2), sublinear_tf=True, max_df=0.7, min_df=3\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:10.751\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36mbuild_preprocessor\u001b[0m:\u001b[36m234\u001b[0m - \u001b[34m\u001b[1mSelectKBest configured: chi2 test, k=4500 features\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_descriptions\u001b[0m:\u001b[36m180\u001b[0m - \u001b[1mTfidfVectorizer fitted: 10000 features extracted (max_features=10000)\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.438\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_descriptions\u001b[0m:\u001b[36m200\u001b[0m - \u001b[32m\u001b[1mDescription features generated: sparse matrix shape (7269, 10000)\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m130\u001b[0m - \u001b[1mGenerating multi-label genre targets...\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m81\u001b[0m - \u001b[1mGenerating multi-label targets from 7269 samples...\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_preprocess_genres\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mStarting genre preprocessing: cleaning and splitting genre strings\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.439\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_preprocess_genres\u001b[0m:\u001b[36m46\u001b[0m - \u001b[34m\u001b[1mFilling missing genres with empty strings\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.442\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_preprocess_genres\u001b[0m:\u001b[36m52\u001b[0m - \u001b[34m\u001b[1mSplitting genre strings by comma and cleaning\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.479\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_preprocess_genres\u001b[0m:\u001b[36m58\u001b[0m - \u001b[32m\u001b[1mGenre preprocessing complete: 7269 samples processed, average 2.64 genres per sample\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.479\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mCreating and fitting new MultiLabelBinarizer\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m89\u001b[0m - \u001b[1mAnalyzing genre frequencies (removing genres < 5.0%)...\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.487\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m102\u001b[0m - \u001b[34m\u001b[1m  Removing 'Music': 2.92% (212 samples)\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.487\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m102\u001b[0m - \u001b[34m\u001b[1m  Removing 'TV Movie': 1.09% (79 samples)\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.487\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m102\u001b[0m - \u001b[34m\u001b[1m  Removing 'War': 3.44% (250 samples)\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.488\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m102\u001b[0m - \u001b[34m\u001b[1m  Removing 'Western': 1.40% (102 samples)\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mIdentified 4 genres to remove: ['Music', 'TV Movie', 'War', 'Western']\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.498\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m119\u001b[0m - \u001b[33m\u001b[1m16 samples lost all genres after filtering and will be removed\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.504\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m125\u001b[0m - \u001b[1mRemoved 16 samples with no genres\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.504\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m128\u001b[0m - \u001b[34m\u001b[1mFitting MultiLabelBinarizer on filtered genre lists\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m132\u001b[0m - \u001b[1mMultiLabelBinarizer fitted: 14 unique genre labels identified\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m133\u001b[0m - \u001b[1mGenres kept: ['Action', 'Adventure', 'Animation', 'Comedy', 'Crime', 'Drama', 'Family', 'Fantasy', 'History', 'Horror', 'Mystery', 'Romance', 'Science Fiction', 'Thriller']\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.512\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m149\u001b[0m - \u001b[32m\u001b[1mTargets generated: shape (7253, 14) (samples × labels)\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.515\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m135\u001b[0m - \u001b[34m\u001b[1mFiltering features to match filtered data: 7253 samples\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m143\u001b[0m - \u001b[1mApplying feature selection with SelectKBest (k=6000)...\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.547\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m146\u001b[0m - \u001b[32m\u001b[1mFeature selection complete: 6000 features selected (from 10000 original features)\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.548\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m156\u001b[0m - \u001b[34m\u001b[1mConverting sparse TF-IDF matrix to dense DataFrame...\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m163\u001b[0m - \u001b[1mFeatures and labels prepared: 7253 samples, 6000 features, 14 labels\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mGenerating TF-IDF features from descriptions...\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_descriptions\u001b[0m:\u001b[36m169\u001b[0m - \u001b[1mGenerating TF-IDF features from 1818 movie descriptions...\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.688\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_descriptions\u001b[0m:\u001b[36m187\u001b[0m - \u001b[34m\u001b[1mUsing pre-fitted TfidfVectorizer for transformation\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_descriptions\u001b[0m:\u001b[36m189\u001b[0m - \u001b[1mTransformed descriptions using existing TfidfVectorizer (10000 features)\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.821\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_descriptions\u001b[0m:\u001b[36m200\u001b[0m - \u001b[32m\u001b[1mDescription features generated: sparse matrix shape (1818, 10000)\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m130\u001b[0m - \u001b[1mGenerating multi-label genre targets...\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m81\u001b[0m - \u001b[1mGenerating multi-label targets from 1818 samples...\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_preprocess_genres\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mStarting genre preprocessing: cleaning and splitting genre strings\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.822\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_preprocess_genres\u001b[0m:\u001b[36m46\u001b[0m - \u001b[34m\u001b[1mFilling missing genres with empty strings\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.823\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_preprocess_genres\u001b[0m:\u001b[36m50\u001b[0m - \u001b[33m\u001b[1mFound 1 samples with missing genres (filled with empty string)\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.824\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_preprocess_genres\u001b[0m:\u001b[36m52\u001b[0m - \u001b[34m\u001b[1mSplitting genre strings by comma and cleaning\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.828\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_preprocess_genres\u001b[0m:\u001b[36m58\u001b[0m - \u001b[32m\u001b[1mGenre preprocessing complete: 1818 samples processed, average 2.68 genres per sample\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.829\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m135\u001b[0m - \u001b[34m\u001b[1mUsing pre-fitted MultiLabelBinarizer for transformation\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m145\u001b[0m - \u001b[1mTransformed labels using existing MultiLabelBinarizer (14 labels)\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.872\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mdescriptions.modeling.preprocess\u001b[0m:\u001b[36m_generate_targets\u001b[0m:\u001b[36m149\u001b[0m - \u001b[32m\u001b[1mTargets generated: shape (1807, 14) (samples × labels)\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.873\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m135\u001b[0m - \u001b[34m\u001b[1mFiltering features to match filtered data: 1807 samples\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.874\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mUsing pre-fitted feature selector for transformation\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1mFeatures transformed: 6000 features\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.876\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m156\u001b[0m - \u001b[34m\u001b[1mConverting sparse TF-IDF matrix to dense DataFrame...\u001b[0m\n",
            "\u001b[32m2025-12-14 20:35:12.909\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdescriptions.modeling.train\u001b[0m:\u001b[36mprepare_features_and_labels\u001b[0m:\u001b[36m163\u001b[0m - \u001b[1mFeatures and labels prepared: 1807 samples, 6000 features, 14 labels\u001b[0m\n",
            "\n",
            "✓ Data prepared:\n",
            "  Training: 7253 samples, 6000 features, 14 labels\n",
            "  Test: 1807 samples, 6000 features, 14 labels\n",
            "  Feature type: float64, Label type: int64\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "data = load_interim(INTERIM_DATA_DIR / \"cleaned_movies.csv\")\n",
        "print(f\"✓ Loaded {len(data)} samples\")\n",
        "\n",
        "# Split data BEFORE preprocessing (prevents data leakage)\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "print(f\"\\nSplitting data (test_size={TEST_SIZE}, random_state={RANDOM_STATE})...\")\n",
        "data_train, data_test = train_test_split(\n",
        "    data, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=True\n",
        ")\n",
        "print(f\"✓ Train: {len(data_train)} samples, Test: {len(data_test)} samples\")\n",
        "\n",
        "# Prepare features and labels using existing pipeline\n",
        "print(\"\\nPreparing features and labels...\")\n",
        "X_train_df, y_train, vectorizer, mlb, feature_selector = prepare_features_and_labels(\n",
        "    data_train, \n",
        "    vectorizer=None, \n",
        "    mlb=None, \n",
        "    feature_selector=None,\n",
        "    k_features=6000  # Use best config from grid search\n",
        ")\n",
        "\n",
        "X_test_df, y_test, _, _, _ = prepare_features_and_labels(\n",
        "    data_test,\n",
        "    vectorizer=vectorizer,\n",
        "    mlb=mlb,\n",
        "    feature_selector=feature_selector\n",
        ")\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_train = X_train_df.values if isinstance(X_train_df, pd.DataFrame) else X_train_df\n",
        "X_test = X_test_df.values if isinstance(X_test_df, pd.DataFrame) else X_test_df\n",
        "\n",
        "print(f\"\\n✓ Data prepared:\")\n",
        "print(f\"  Training: {X_train.shape[0]} samples, {X_train.shape[1]} features, {y_train.shape[1]} labels\")\n",
        "print(f\"  Test: {X_test.shape[0]} samples, {X_test.shape[1]} features, {y_test.shape[1]} labels\")\n",
        "print(f\"  Feature type: {X_train.dtype}, Label type: {y_train.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create PyTorch Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Training dataset created: 7253 samples\n",
            "  Feature dimension: 6000\n",
            "  Number of labels: 14\n",
            "\n",
            "✓ Test dataset created: 1807 samples\n",
            "\n",
            "✓ Validation split created:\n",
            "  Final training: 5802 samples\n",
            "  Validation: 1451 samples\n",
            "\n",
            "✓ Sample data shape:\n",
            "  Features: torch.Size([6000])\n",
            "  Labels: torch.Size([14])\n",
            "  Feature dtype: torch.float32, Label dtype: torch.float32\n"
          ]
        }
      ],
      "source": [
        "# Create training dataset\n",
        "train_dataset = MovieGenreDataset(X_train, y_train)\n",
        "print(f\"✓ Training dataset created: {len(train_dataset)} samples\")\n",
        "print(f\"  Feature dimension: {train_dataset.get_feature_dim()}\")\n",
        "print(f\"  Number of labels: {train_dataset.get_num_labels()}\")\n",
        "\n",
        "# Create test dataset\n",
        "test_dataset = MovieGenreDataset(X_test, y_test)\n",
        "print(f\"\\n✓ Test dataset created: {len(test_dataset)} samples\")\n",
        "\n",
        "# Create validation dataset (split from training)\n",
        "VAL_SIZE = 0.2\n",
        "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=VAL_SIZE, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "train_dataset_final = MovieGenreDataset(X_train_final, y_train_final)\n",
        "val_dataset = MovieGenreDataset(X_val, y_val)\n",
        "\n",
        "print(f\"\\n✓ Validation split created:\")\n",
        "print(f\"  Final training: {len(train_dataset_final)} samples\")\n",
        "print(f\"  Validation: {len(val_dataset)} samples\")\n",
        "\n",
        "# Test dataset indexing\n",
        "sample_features, sample_labels = train_dataset[0]\n",
        "print(f\"\\n✓ Sample data shape:\")\n",
        "print(f\"  Features: {sample_features.shape}\")\n",
        "print(f\"  Labels: {sample_labels.shape}\")\n",
        "print(f\"  Feature dtype: {sample_features.dtype}, Label dtype: {sample_labels.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ DataLoaders created:\n",
            "  Training batches: 182 (batch_size=32)\n",
            "  Validation batches: 46\n",
            "  Test batches: 57\n",
            "\n",
            "✓ Sample batch:\n",
            "  Features shape: torch.Size([32, 6000]) (batch_size, n_features)\n",
            "  Labels shape: torch.Size([32, 14]) (batch_size, n_labels)\n",
            "  On device: cpu\n"
          ]
        }
      ],
      "source": [
        "# DataLoader parameters\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 0  # Set to 0 for Windows/Mac, can use 2-4 on Linux\n",
        "\n",
        "# Create training DataLoader with shuffling\n",
        "train_loader = DataLoader(\n",
        "    train_dataset_final,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,  # Shuffle for training\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True if torch.cuda.is_available() else False  # Faster GPU transfer\n",
        ")\n",
        "\n",
        "# Create validation DataLoader (no shuffling needed)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,  # No shuffle for validation\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "# Create test DataLoader (no shuffling needed)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,  # No shuffle for testing\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "print(\"✓ DataLoaders created:\")\n",
        "print(f\"  Training batches: {len(train_loader)} (batch_size={BATCH_SIZE})\")\n",
        "print(f\"  Validation batches: {len(val_loader)}\")\n",
        "print(f\"  Test batches: {len(test_loader)}\")\n",
        "\n",
        "# Test a batch\n",
        "sample_batch_features, sample_batch_labels = next(iter(train_loader))\n",
        "print(f\"\\n✓ Sample batch:\")\n",
        "print(f\"  Features shape: {sample_batch_features.shape} (batch_size, n_features)\")\n",
        "print(f\"  Labels shape: {sample_batch_labels.shape} (batch_size, n_labels)\")\n",
        "print(f\"  On device: {sample_batch_features.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Example: Simple Feedforward Model\n",
        "\n",
        "Ready-to-use model architecture for multi-label classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Model created:\n",
            "  Architecture: 6000 -> [512, 256, 128] -> 14\n",
            "  Total parameters: 3,240,334\n",
            "  Trainable parameters: 3,240,334\n",
            "  Model on device: cpu\n",
            "\n",
            "✓ Forward pass test:\n",
            "  Input shape: torch.Size([32, 6000])\n",
            "  Output shape: torch.Size([32, 14])\n",
            "  Output range: [-4.48, 4.12]\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "class GenreClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-label genre classifier using feedforward neural network.\n",
        "    \n",
        "    Architecture:\n",
        "    - Input: TF-IDF features\n",
        "    - Hidden layers: Fully connected with ReLU and Dropout\n",
        "    - Output: Logits for each genre (sigmoid applied in loss function)\n",
        "    \n",
        "    Note: BatchNorm removed to prevent kernel crashes. Dropout provides sufficient regularization.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int,\n",
        "        num_labels: int,\n",
        "        hidden_sizes: list = [512, 256, 128],\n",
        "        dropout_rate: float = 0.3,\n",
        "    ):\n",
        "        super(GenreClassifier, self).__init__()\n",
        "        \n",
        "        layers = []\n",
        "        prev_size = input_size\n",
        "        \n",
        "        # Build hidden layers\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.append(nn.Linear(prev_size, hidden_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout_rate))\n",
        "            # BatchNorm removed to prevent kernel crashes - dropout provides sufficient regularization\n",
        "            prev_size = hidden_size\n",
        "        \n",
        "        # Output layer (no activation - BCEWithLogitsLoss handles sigmoid)\n",
        "        layers.append(nn.Linear(prev_size, num_labels))\n",
        "        \n",
        "        self.network = nn.Sequential(*layers)\n",
        "        \n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "    \n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize network weights using Xavier uniform.\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        \n",
        "        Args:\n",
        "            x: Input features (batch_size, input_size)\n",
        "        \n",
        "        Returns:\n",
        "            Logits for each label (batch_size, num_labels)\n",
        "        \"\"\"\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "input_size = train_dataset.get_feature_dim()\n",
        "num_labels = train_dataset.get_num_labels()\n",
        "\n",
        "model = GenreClassifier(\n",
        "    input_size=input_size,\n",
        "    num_labels=num_labels,\n",
        "    hidden_sizes=[512, 256, 128],\n",
        "    dropout_rate=0.3\n",
        ").to(device)\n",
        "\n",
        "print(f\"✓ Model created:\")\n",
        "print(f\"  Architecture: {input_size} -> [512, 256, 128] -> {num_labels}\")\n",
        "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "print(f\"  Model on device: {next(model.parameters()).device}\")\n",
        "\n",
        "# Test forward pass (ensure model is in eval mode)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_output = model(sample_batch_features.to(device))\n",
        "    print(f\"\\n✓ Forward pass test:\")\n",
        "    print(f\"  Input shape: {sample_batch_features.shape}\")\n",
        "    print(f\"  Output shape: {test_output.shape}\")\n",
        "    print(f\"  Output range: [{test_output.min().item():.2f}, {test_output.max().item():.2f}]\")\n",
        "model.train()  # Set back to training mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Setup (Ready to Use)\n",
        "\n",
        "Loss function, optimizer, and training loop structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss function - BCEWithLogitsLoss for multi-label classification\n",
        "# This combines sigmoid + BCE loss for numerical stability\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Optimizer\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0.0001  # L2 regularization\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',  # Minimize validation loss\n",
        "    factor=0.5,  # Reduce LR by half\n",
        "    patience=5,  # Wait 5 epochs before reducing\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"✓ Training setup complete:\")\n",
        "print(f\"  Loss function: BCEWithLogitsLoss (multi-label binary cross-entropy)\")\n",
        "print(f\"  Optimizer: Adam (lr={LEARNING_RATE}, weight_decay={WEIGHT_DECAY})\")\n",
        "print(f\"  Scheduler: ReduceLROnPlateau (factor=0.5, patience=5)\")\n",
        "\n",
        "# Test loss calculation\n",
        "with torch.no_grad():\n",
        "    test_output = model(sample_batch_features.to(device))\n",
        "    test_loss = criterion(test_output, sample_batch_labels.to(device))\n",
        "    print(f\"\\n✓ Loss calculation test:\")\n",
        "    print(f\"  Sample batch loss: {test_loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training Loop Template\n",
        "\n",
        "Ready-to-use training loop with validation and metrics calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch_features, batch_labels in train_loader:\n",
        "        batch_features = batch_features.to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_features)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    return total_loss / num_batches\n",
        "\n",
        "\n",
        "def validate(model, val_loader, criterion, device, threshold=0.5):\n",
        "    \"\"\"Validate model and calculate metrics.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_features, batch_labels in val_loader:\n",
        "            batch_features = batch_features.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "            \n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            # Convert logits to probabilities and then to binary predictions\n",
        "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
        "            preds = (probs >= threshold).astype(int)\n",
        "            \n",
        "            all_preds.append(preds)\n",
        "            all_labels.append(batch_labels.cpu().numpy())\n",
        "    \n",
        "    # Concatenate all predictions\n",
        "    all_preds = np.vstack(all_preds)\n",
        "    all_labels = np.vstack(all_labels)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    f1 = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n",
        "    precision = precision_score(all_labels, all_preds, average='micro', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='micro', zero_division=0)\n",
        "    hamming = hamming_loss(all_labels, all_preds)\n",
        "    jaccard = jaccard_score(all_labels, all_preds, average='micro', zero_division=0)\n",
        "    \n",
        "    metrics = {\n",
        "        'loss': avg_loss,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'hamming_loss': hamming,\n",
        "        'jaccard': jaccard\n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "\n",
        "# Example training loop (commented out - uncomment to train)\n",
        "EPOCHS = 50\n",
        "PATIENCE = 10\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "history = {'train_loss': [], 'val_loss': [], 'val_f1': []}\n",
        "\n",
        "print(f\"Starting training for {EPOCHS} epochs...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Training\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    \n",
        "    # Validation\n",
        "    val_metrics = validate(model, val_loader, criterion, device)\n",
        "    \n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_metrics['loss'])\n",
        "    \n",
        "    # Track history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_metrics['loss'])\n",
        "    history['val_f1'].append(val_metrics['f1'])\n",
        "    \n",
        "    # Early stopping\n",
        "    if val_metrics['loss'] < best_val_loss:\n",
        "        best_val_loss = val_metrics['loss']\n",
        "        patience_counter = 0\n",
        "        # Save best model\n",
        "        torch.save(model.state_dict(), MODELS_DIR / 'pytorch_best_model.pt')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "            model.load_state_dict(torch.load(MODELS_DIR / 'pytorch_best_model.pt'))\n",
        "            break\n",
        "    \n",
        "    # Print progress\n",
        "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "        print(f\"Epoch {epoch+1:3d}/{EPOCHS} | \"\n",
        "              f\"Train Loss: {train_loss:.4f} | \"\n",
        "              f\"Val Loss: {val_metrics['loss']:.4f} | \"\n",
        "              f\"Val F1: {val_metrics['f1']:.4f} | \"\n",
        "              f\"Val Precision: {val_metrics['precision']:.4f} | \"\n",
        "              f\"Val Recall: {val_metrics['recall']:.4f})\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"✓ Training functions defined:\")\n",
        "print(\"  - train_epoch(): Train for one epoch\")\n",
        "print(\"  - validate(): Validate and calculate metrics\")\n",
        "print(\"  - Training loop template ready (commented out)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Quick Test: Single Epoch\n",
        "\n",
        "Test the training setup with one epoch to ensure everything works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test training for one epoch\n",
        "print(\"Testing training setup with one epoch...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Train one epoch\n",
        "train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "print(f\"✓ Training epoch complete: Loss = {train_loss:.4f}\")\n",
        "\n",
        "# Validate\n",
        "val_metrics = validate(model, val_loader, criterion, device, threshold=0.5)\n",
        "print(f\"\\n✓ Validation complete:\")\n",
        "print(f\"  Loss: {val_metrics['loss']:.4f}\")\n",
        "print(f\"  F1: {val_metrics['f1']:.4f} ({val_metrics['f1']*100:.2f}%)\")\n",
        "print(f\"  Precision: {val_metrics['precision']:.4f} ({val_metrics['precision']*100:.2f}%)\")\n",
        "print(f\"  Recall: {val_metrics['recall']:.4f} ({val_metrics['recall']*100:.2f}%)\")\n",
        "print(f\"  Hamming Loss: {val_metrics['hamming_loss']:.4f}\")\n",
        "print(f\"  Jaccard: {val_metrics['jaccard']:.4f} ({val_metrics['jaccard']*100:.2f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✓ Dataset and DataLoader setup complete and tested!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Uncomment the training loop in cell 15 to train the model\")\n",
        "print(\"2. Adjust hyperparameters (hidden_sizes, dropout_rate, learning_rate)\")\n",
        "print(\"3. Experiment with different architectures (GRU, LSTM)\")\n",
        "print(\"4. Compare with sklearn LinearSVC performance\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "movie_genre_model",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
