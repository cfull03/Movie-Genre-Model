{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1f6344c-4fcb-441d-9441-e1b2f05be4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from typing import Dict, Any, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8beb4909-8c71-4dc1-82f4-15d43b17c03d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movie_name     0\n",
       "genre          3\n",
       "description    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "movies_df = pd.read_csv('top_movies.csv')\n",
    "movies_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec933c83-a14f-4c98-9940-a24c694fe428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movie_name     0\n",
       "genre          0\n",
       "description    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df = movies_df.dropna()\n",
    "movies_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70c83de8-4013-41ec-97ca-8fe8c628ae40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Total empty sequences found: 0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "empty_sequences = []\n",
    "\n",
    "for idx, description in enumerate(movies_df['description'].tolist()):\n",
    "    encoding = tokenizer(\n",
    "        description,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "    if attention_mask.sum().item() == 0:\n",
    "        print(f\"⚠️ Empty sequence found at index {idx}: {description}\")\n",
    "        empty_sequences.append(idx)\n",
    "\n",
    "print(f\"\\n✅ Total empty sequences found: {len(empty_sequences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2d01ff7-8a19-482d-9204-c6a39aff0077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre to index mapping: {'Action': 0, 'Adventure': 1, 'Animation': 2, 'Comedy': 3, 'Crime': 4, 'Drama': 5, 'Family': 6, 'Fantasy': 7, 'History': 8, 'Horror': 9, 'Music': 10, 'Mystery': 11, 'Romance': 12, 'Science Fiction': 13, 'TV Movie': 14, 'Thriller': 15, 'War': 16, 'Western': 17}\n"
     ]
    }
   ],
   "source": [
    "all_genres = set()\n",
    "for genres in movies_df['genre']:\n",
    "    for genre in genres.split(','):\n",
    "        all_genres.add(genre.strip())\n",
    "        \n",
    "genre_to_index = {genre: idx for idx, genre in enumerate(sorted(all_genres))}\n",
    "\n",
    "print(\"Genre to index mapping:\", genre_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45c54bea-4399-4631-a1e8-8cfae2205329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDescriptionDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, genre_to_index: Dict[str, int], max_length: int = 128):\n",
    "        self.descriptions = dataframe['description'].tolist()\n",
    "        self.genres = dataframe['genre'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.genre_to_index = genre_to_index\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.descriptions)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        description = self.descriptions[idx]\n",
    "        genre_string = self.genres[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            description,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        label = torch.zeros(len(self.genre_to_index))\n",
    "        for genre in genre_string.split(','):\n",
    "            genre = genre.strip()\n",
    "            if genre in self.genre_to_index:\n",
    "                label[self.genre_to_index[genre]] = 1\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db4ad6f8-47ff-4076-9972-d23b7c18211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(movies_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1780ee0-d3e3-4586-9831-d5a922fea4c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7533\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MovieDescriptionDataset(train_df, tokenizer, genre_to_index, max_length=128)\n",
    "val_dataset = MovieDescriptionDataset(val_df, tokenizer, genre_to_index, max_length=128)\n",
    "\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa194d91-38f1-440d-8c2f-01badb05c850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([  101,  1999,  1996,  3865,  1010,  1037,  7101,  2003,  4704,  2011,\n",
      "         2010,  2316,  2074,  2077,  2027,  2468,  2600, 18795,  2015,  1012,\n",
      "         3174,  2086,  2101,  1010,  1996,  7101,  5927,  2010,  2117,  3382,\n",
      "         2012,  2732,  9527, 13368,  2043,  2002,  2003,  2356,  2000,  4685,\n",
      "         2007,  2010,  9454,  7833,  1005,  1055,  2152,  2082,  2600,  2316,\n",
      "         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Labels: tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "sample = train_dataset[0]\n",
    "\n",
    "print(\"Input IDs:\", sample['input_ids'])\n",
    "print(\"Attention Mask:\", sample['attention_mask'])\n",
    "print(\"Labels:\", sample['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7157935-0720-48a6-bf53-02e36dcc0530",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b4a662a-5d9f-4088-b0f3-8c55247d35f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionPooling(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.attention_vector = nn.Parameter(torch.randn(emb_dim))\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        # x: [batch_size, seq_length, emb_dim]\n",
    "        scores = torch.matmul(x, self.attention_vector)  # [batch_size, seq_length]\n",
    "        scores = scores.masked_fill(~attention_mask.bool(), float('-inf'))  # Mask out padding tokens\n",
    "        attention_weights = torch.softmax(scores, dim=1).unsqueeze(-1)  # [batch_size, seq_length, 1]\n",
    "\n",
    "        pooled = (x * attention_weights).sum(dim=1)  # Weighted sum: [batch_size, emb_dim]\n",
    "        return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29bac456-25a4-4923-96d6-c74e92166668",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        emb_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_classes: int,\n",
    "        num_heads: int = 8,\n",
    "        max_seq_length: int = 512,\n",
    "        num_attention_layers: int = 4, \n",
    "        feedforward_dim: int = 256,\n",
    "        num_dropout_samples: int = 8\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_dropout_samples = num_dropout_samples\n",
    "        self.mc_dropout_enabled = True \n",
    "\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_length, emb_dim))\n",
    "\n",
    "        self.attention_pooling = SelfAttentionPooling(emb_dim)\n",
    "\n",
    "        transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=feedforward_dim,\n",
    "            dropout=0.3,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            transformer_layer,\n",
    "            num_layers=num_attention_layers\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(emb_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, num_classes)\n",
    "\n",
    "        self.layernorm_final = nn.LayerNorm(hidden_dim // 2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor, debugging: bool = False) -> torch.Tensor:\n",
    "        if debugging:\n",
    "            print(f\"Input shape after embedding input IDs: {x.shape}\")\n",
    "\n",
    "        x = self.emb(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=~attention_mask.bool())\n",
    "\n",
    "        if debugging:\n",
    "            print(f\"Output shape after transformer: {x.shape}\")\n",
    "\n",
    "        x = self.attention_pooling(x, attention_mask)\n",
    "\n",
    "        if debugging:\n",
    "            print(f\"Output shape after attention pooling: {x.shape}\")\n",
    "\n",
    "        residual = x\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.layernorm_final(x + residual)\n",
    "\n",
    "        if debugging:\n",
    "            print(f\"Shape before final classifier: {x.shape}\")\n",
    "\n",
    "        if self.training and self.mc_dropout_enabled:\n",
    "            logits_list = []\n",
    "            for _ in range(self.num_dropout_samples):\n",
    "                dropped = self.dropout(x)\n",
    "                logits = self.fc3(dropped)\n",
    "                logits_list.append(logits)\n",
    "\n",
    "            if debugging:\n",
    "                print(f\"Shape of each logits before stacking: {logits.shape}\")\n",
    "\n",
    "            logits = torch.stack(logits_list, dim=0).mean(dim=0)\n",
    "\n",
    "            if debugging:\n",
    "                print(f\"Final logits shape after averaging dropout samples: {logits.shape}\")\n",
    "        else:\n",
    "            logits = self.fc3(x)\n",
    "            if debugging:\n",
    "                print(f\"Final logits shape (single pass): {logits.shape}\")\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20e6c8f6-312c-473b-b0ac-33bf025996fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma: float = 2.0, pos_weight: Optional[torch.Tensor] = None, reduction: str = 'mean') -> None:\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.pos_weight = pos_weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logits, targets, pos_weight=self.pos_weight, reduction='none')\n",
    "        \n",
    "        probs = torch.sigmoid(logits)\n",
    "        probs = torch.clamp(probs, min=1e-6, max=1 - 1e-6)\n",
    "\n",
    "        focal_weight = torch.where(targets == 1, 1 - probs, probs) ** self.gamma\n",
    "        loss = focal_weight * bce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc9bfbd9-81bb-47b6-bacb-74b8c27acff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "119d977a-a1cf-4371-8b0e-d3191eb21f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30522\n",
    "emb_dim = 128\n",
    "hidden_dim = 256\n",
    "num_heads = 8\n",
    "\n",
    "num_classes = len(genre_to_index)\n",
    "max_seq_length = 512\n",
    "num_attention_layers = 4\n",
    "feedforward_dim = 256\n",
    "num_dropout_samples = 5\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9455f1cc-7443-4375-bc6d-fdd0c9e6f3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_dim=emb_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_classes=num_classes,\n",
    "    num_heads=num_heads,\n",
    "    max_seq_length=max_seq_length,\n",
    "    num_attention_layers=num_attention_layers,\n",
    "    feedforward_dim=feedforward_dim,\n",
    "    num_dropout_samples=num_dropout_samples\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d5d623c-33db-4654-afd0-5c3b1526502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = FocalLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b771bdc-c06d-4b20-85b5-021fea3dcde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: 100%|██████████████| 942/942 [02:15<00:00,  6.95it/s, loss=0.0835]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:384: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:179.)\n",
      "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "Train Loss: 0.1063 | Val Loss: 0.0961\n",
      "Train Precision: 0.2884 | Val Precision: 0.3710\n",
      "Train Recall:    0.4203    | Val Recall:    0.3742\n",
      "Train F1 Score:  0.3421        | Val F1 Score:  0.3726\n",
      "Train Subset Acc: 0.0046 | Val Subset Acc: 0.0111\n",
      "\n",
      "Epoch [2/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]: 100%|█████████████████| 942/942 [02:17<00:00,  6.83it/s, loss=0.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "Train Loss: 0.1002 | Val Loss: 0.0952\n",
      "Train Precision: 0.3208 | Val Precision: 0.3659\n",
      "Train Recall:    0.4249    | Val Recall:    0.4659\n",
      "Train F1 Score:  0.3656        | Val F1 Score:  0.4099\n",
      "Train Subset Acc: 0.0085 | Val Subset Acc: 0.0064\n",
      "\n",
      "Epoch [3/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]: 100%|██████████████| 942/942 [02:19<00:00,  6.73it/s, loss=0.0897]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "Train Loss: 0.0982 | Val Loss: 0.0946\n",
      "Train Precision: 0.3391 | Val Precision: 0.3861\n",
      "Train Recall:    0.4390    | Val Recall:    0.4048\n",
      "Train F1 Score:  0.3827        | Val F1 Score:  0.3952\n",
      "Train Subset Acc: 0.0110 | Val Subset Acc: 0.0308\n",
      "\n",
      "Epoch [4/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]: 100%|███████████████| 942/942 [02:24<00:00,  6.50it/s, loss=0.073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "Train Loss: 0.0948 | Val Loss: 0.0906\n",
      "Train Precision: 0.3654 | Val Precision: 0.3952\n",
      "Train Recall:    0.4816    | Val Recall:    0.4985\n",
      "Train F1 Score:  0.4155        | Val F1 Score:  0.4409\n",
      "Train Subset Acc: 0.0232 | Val Subset Acc: 0.0265\n",
      "\n",
      "Epoch [5/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]: 100%|██████████████| 942/942 [02:33<00:00,  6.14it/s, loss=0.0863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "Train Loss: 0.0915 | Val Loss: 0.0881\n",
      "Train Precision: 0.3899 | Val Precision: 0.4079\n",
      "Train Recall:    0.5233    | Val Recall:    0.5405\n",
      "Train F1 Score:  0.4468        | Val F1 Score:  0.4650\n",
      "Train Subset Acc: 0.0264 | Val Subset Acc: 0.0287\n",
      "\n",
      "Epoch [6/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]: 100%|██████████████| 942/942 [02:27<00:00,  6.38it/s, loss=0.0706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary:\n",
      "Train Loss: 0.0883 | Val Loss: 0.0863\n",
      "Train Precision: 0.4081 | Val Precision: 0.4188\n",
      "Train Recall:    0.5587    | Val Recall:    0.5815\n",
      "Train F1 Score:  0.4717        | Val F1 Score:  0.4869\n",
      "Train Subset Acc: 0.0297 | Val Subset Acc: 0.0366\n",
      "\n",
      "Epoch [7/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]: 100%|██████████████| 942/942 [02:35<00:00,  6.08it/s, loss=0.0726]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Summary:\n",
      "Train Loss: 0.0856 | Val Loss: 0.0851\n",
      "Train Precision: 0.4230 | Val Precision: 0.4230\n",
      "Train Recall:    0.5974    | Val Recall:    0.5805\n",
      "Train F1 Score:  0.4953        | Val F1 Score:  0.4894\n",
      "Train Subset Acc: 0.0350 | Val Subset Acc: 0.0377\n",
      "\n",
      "Epoch [8/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]: 100%|██████████████| 942/942 [02:33<00:00,  6.15it/s, loss=0.0834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Summary:\n",
      "Train Loss: 0.0832 | Val Loss: 0.0833\n",
      "Train Precision: 0.4378 | Val Precision: 0.4344\n",
      "Train Recall:    0.6196    | Val Recall:    0.6172\n",
      "Train F1 Score:  0.5131        | Val F1 Score:  0.5099\n",
      "Train Subset Acc: 0.0362 | Val Subset Acc: 0.0398\n",
      "\n",
      "Epoch [9/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]: 100%|██████████████| 942/942 [02:26<00:00,  6.44it/s, loss=0.0826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Summary:\n",
      "Train Loss: 0.0805 | Val Loss: 0.0825\n",
      "Train Precision: 0.4494 | Val Precision: 0.4364\n",
      "Train Recall:    0.6448    | Val Recall:    0.6406\n",
      "Train F1 Score:  0.5296        | Val F1 Score:  0.5191\n",
      "Train Subset Acc: 0.0353 | Val Subset Acc: 0.0377\n",
      "\n",
      "Epoch [10/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]: 100%|██████████████| 942/942 [02:38<00:00,  5.96it/s, loss=0.104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 Summary:\n",
      "Train Loss: 0.0785 | Val Loss: 0.0828\n",
      "Train Precision: 0.4631 | Val Precision: 0.4316\n",
      "Train Recall:    0.6634    | Val Recall:    0.6448\n",
      "Train F1 Score:  0.5455        | Val F1 Score:  0.5171\n",
      "Train Subset Acc: 0.0416 | Val Subset Acc: 0.0382\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "threshold = 0.4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch [{epoch + 1}/{num_epochs}]\")\n",
    "\n",
    "    # 🔹 Training Phase\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_train_labels = []\n",
    "    all_train_preds = []\n",
    "\n",
    "    loop = tqdm(train_dataloader, leave=True)\n",
    "\n",
    "    for batch in loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)  # ✅ Get the mask\n",
    "        labels = batch['labels'].to(device).float()\n",
    "\n",
    "        # ✅ Pass the mask to the model\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        probs = torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "        preds = (probs >= threshold).astype(int)\n",
    "\n",
    "        all_train_labels.extend(labels.cpu().numpy())\n",
    "        all_train_preds.extend(preds)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    train_precision = precision_score(all_train_labels, all_train_preds, average='micro', zero_division=0)\n",
    "    train_recall = recall_score(all_train_labels, all_train_preds, average='micro', zero_division=0)\n",
    "    train_f1 = f1_score(all_train_labels, all_train_preds, average='micro', zero_division=0)\n",
    "    train_subset_acc = accuracy_score(all_train_labels, all_train_preds)\n",
    "\n",
    "    # 🔹 Validation Phase (Integrated)\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_val_labels = []\n",
    "    all_val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device).float()\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "            preds = (probs >= threshold).astype(int)\n",
    "\n",
    "            all_val_labels.extend(labels.cpu().numpy())\n",
    "            all_val_preds.extend(preds)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    val_precision = precision_score(all_val_labels, all_val_preds, average='micro', zero_division=0)\n",
    "    val_recall = recall_score(all_val_labels, all_val_preds, average='micro', zero_division=0)\n",
    "    val_f1 = f1_score(all_val_labels, all_val_preds, average='micro', zero_division=0)\n",
    "    val_subset_acc = accuracy_score(all_val_labels, all_val_preds)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Train Precision: {train_precision:.4f} | Val Precision: {val_precision:.4f}\")\n",
    "    print(f\"Train Recall:    {train_recall:.4f}    | Val Recall:    {val_recall:.4f}\")\n",
    "    print(f\"Train F1 Score:  {train_f1:.4f}        | Val F1 Score:  {val_f1:.4f}\")\n",
    "    print(f\"Train Subset Acc: {train_subset_acc:.4f} | Val Subset Acc: {val_subset_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7714fe-cab8-4687-b1b1-19ac9099ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_genres(description, model, tokenizer, genre_to_index, threshold=0.5, device='cuda'):\n",
    "    model.eval()\n",
    "    index_to_genre = {v: k for k, v in genre_to_index.items()}\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        description,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "\n",
    "    predicted_labels = (probs >= threshold).astype(int)\n",
    "    predicted_genres = [index_to_genre[i] for i, label in enumerate(predicted_labels[0]) if label == 1]\n",
    "\n",
    "    return predicted_genres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8740d4cd-9c0b-44c9-a13c-9422a0176420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
